{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "# import optuna.integration.lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feather(filename):\n",
    "    df = pd.read_feather(os.path.join(INPUT_DIR, filename))\n",
    "    print(filename, df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.f (1997595, 35)\n",
      "test.f (390095, 30)\n"
     ]
    }
   ],
   "source": [
    "train_df = read_feather('train.f')\n",
    "test_df = read_feather('test.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaign.f (14627, 4)\n",
      "map_game_feed_native_video_assets.f (2796, 3)\n",
      "advertiser_video.f (11707, 6)\n",
      "advertiser_converted_video.f (198622, 8)\n"
     ]
    }
   ],
   "source": [
    "campaign_df = read_feather('campaign.f')\n",
    "map_gv_df = read_feather('map_game_feed_native_video_assets.f')\n",
    "ad_video_df = read_feather('advertiser_video.f')\n",
    "ad_cvideo_df = read_feather('advertiser_converted_video.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_cvideo_df (107493, 8)\n"
     ]
    }
   ],
   "source": [
    "ad_cvideo_df = ad_cvideo_df.drop_duplicates(\n",
    "    subset=['mst_advertiser_video_id', \n",
    "                   'mst_game_feed_id', \n",
    "                    'mst_video_template_id'], keep='last')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['vertical', 'horizontal'])\n",
    "ad_cvideo_df['rectangle_type_id'] = le.transform(ad_cvideo_df['rectangle_type'])\n",
    "ad_cvideo_df.drop(columns=['rectangle_type'], inplace=True)\n",
    "print('ad_cvideo_df', ad_cvideo_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding_col = [\"adspot_id\", \"adspot_video_format_id\", \n",
    "               \"country_code\", \"game_feed_asset_type_id\",\n",
    "               \"item_id\", \"os\", \"uid\", \"user_type_id\", \"video_template_id\", \"category_id\", \"game_feed_id\", \"game_template_id\"]\n",
    "\n",
    "# \"advertiser_id\", \"app_id\", \"media_app_id\", \"campaign_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_overlapping(column: str):\n",
    "    \"\"\"train/testにしか出てこない値を調べる\"\"\"\n",
    "    only_in_train = set(train_df[column].unique()) - set(test_df[column].unique())\n",
    "    only_in_test = set(test_df[column].unique()) - set(train_df[column].unique())\n",
    "    non_overlapping = only_in_train.union(only_in_test)\n",
    "    return non_overlapping\n",
    "\n",
    "def category2num(input_df, columns: list):\n",
    "    input_ = input_df[columns].copy()\n",
    "    for column in columns:\n",
    "        non_overlapping = get_non_overlapping(column)\n",
    "        if input_df[column].dtype == np.dtype(\"O\"):\n",
    "            # dtypeがobjectなら欠損は'missing' クラスにする\n",
    "            input_[column] = input_df[column].fillna(\"missing\")\n",
    "            input_[column] = input_[column].map(lambda x: x if x not in non_overlapping else \"other\")\n",
    "        else:\n",
    "            # dtypeがint/floatなら欠損は'-1'とする\n",
    "            input_[column] = input_df[column].fillna(-1)\n",
    "            input_[column] = input_[column].map(lambda x: x if x not in non_overlapping else -2)\n",
    "\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_LE (1997595, 12)\n",
      "test_LE (390095, 12)\n"
     ]
    }
   ],
   "source": [
    "train_LE = category2num(train_df, label_encoding_col)\n",
    "print('train_LE', train_LE.shape)\n",
    "test_LE = category2num(test_df, label_encoding_col)\n",
    "print('test_LE', test_LE.shape)\n",
    "concatenated = pd.concat([train_LE, test_LE], axis=0).reset_index(drop=True)\n",
    "\n",
    "for column in label_encoding_col:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(concatenated[column])\n",
    "    train_LE[column] = le.transform(train_LE[column])\n",
    "    test_LE[column] = le.transform(test_LE[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoding_features(input_df, is_test=False):\n",
    "    if is_test:\n",
    "        return test_LE\n",
    "    else:\n",
    "        return train_LE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc_col = ['advertiser_id', 'app_id', 'media_app_id', 'campaign_id']\n",
    "# , 'category_id', 'game_feed_id', 'game_template_id', 'item_id', 'uid', 'user_type_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(train, test, enc_col):\n",
    "    group_train = train.groupby([enc_col]).mean()[['target']].reset_index()\n",
    "    test_copy = test[[enc_col]].copy()\n",
    "    test_merge = pd.merge(test_copy, group_train, on=[enc_col], how='left')\n",
    "    test_merge.set_index(test_copy.index, inplace=True)\n",
    "    test_merge['target'].fillna(train['target'].mean(), inplace=True)\n",
    "    enc_name = 'TGE_' + enc_col\n",
    "    test_merge.rename(columns={'target': enc_name}, inplace=True)\n",
    "    return test_merge.drop(columns=enc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_targetencoding_features(input_df, is_test=False):\n",
    "    \n",
    "    if is_test:\n",
    "        # test用 (全データ使用)\n",
    "        print('for test')\n",
    "        tgt_all = pd.DataFrame()\n",
    "        for enc_col in target_enc_col:\n",
    "            tmp = target_encoding(train_df, test_df, enc_col)\n",
    "            tgt_all = pd.concat([tgt_all, tmp], axis=1)\n",
    "        return tgt_all\n",
    "    \n",
    "    else:\n",
    "        # train用 (oof)\n",
    "        print('for train')\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "        tgt_all = pd.DataFrame()\n",
    "        for enc_col in target_enc_col:\n",
    "            tgt_col = pd.DataFrame()\n",
    "\n",
    "            for train_index, eval_index in kf.split(train_df):\n",
    "                kf_train = train_df.iloc[train_index]\n",
    "                kf_eval = train_df.iloc[eval_index]\n",
    "\n",
    "                tmp = target_encoding(kf_train, kf_eval, enc_col)\n",
    "                tgt_col = pd.concat([tgt_col, tmp])\n",
    "\n",
    "            tgt_all = pd.concat([tgt_all, tgt_col], axis=1)\n",
    "            print(enc_col)\n",
    "\n",
    "        return tgt_all.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_encoding_features(input_df, is_test=False):\n",
    "    use_columns = [\n",
    "        'category_id', \n",
    "        'adnw_id', \n",
    "        'adspot_id', \n",
    "        'app_id', \n",
    "        'advertiser_id',\n",
    "        'uid', \n",
    "        'game_feed_id'\n",
    "    ]\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for c in use_columns:\n",
    "        series = whole_df[c]\n",
    "        vc = series.value_counts(dropna=False)\n",
    "\n",
    "        _df = pd.DataFrame(input_df[c].map(vc))\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "\n",
    "    out_df = out_df.add_prefix('CE_')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_encoding_features(input_df, is_test=False):\n",
    "    use_columns = [\n",
    "        # カーディナリティが比較的低めなものに絞っています\n",
    "        'auction_type_id', 'header_bidding', 'is_interstitial', 'user_type_id' # and more\n",
    "    ]\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    for c in use_columns:\n",
    "        series = input_df[c]\n",
    "        cat = pd.Categorical(series, categories=whole_df[c].dropna().unique())\n",
    "        _df = pd.get_dummies(cat)\n",
    "        _df.columns = _df.columns.tolist()\n",
    "        _df = _df.add_prefix(c + '=')\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "\n",
    "    return out_df.add_prefix('OH_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集約系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregate_features(input_df, is_test=False):\n",
    "    return _run_aggregation(input_df, 'uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_aggregation(input_df, agg_column):\n",
    "    _agg_df = pd.concat([\n",
    "        whole_df.groupby(agg_column)['last_login_interval'].agg(['mean', 'min', 'max']).add_prefix('last_login_int_'),\n",
    "        whole_df.groupby(agg_column)['first_login_interval'].agg(['mean', 'min', 'max']).add_prefix('first_login_int_'),\n",
    "        whole_df.groupby(agg_column)['max_login_interval'].agg(['mean', 'min', 'max']).add_prefix('max_login_int_'),\n",
    "        whole_df.groupby(agg_column)['frequency'].agg(['mean', 'min', 'max']).add_prefix('frequency_'),\n",
    "        whole_df.groupby(agg_column)['login_frequency'].agg(['mean', 'min', 'max']).add_prefix('login_frequency_'),\n",
    "        whole_df.groupby(agg_column)['advertiser_id'].nunique(),\n",
    "        whole_df.groupby(agg_column)['app_id'].nunique(),\n",
    "        whole_df.groupby(agg_column)['media_app_id'].nunique(),\n",
    "        whole_df.groupby(agg_column)['campaign_id'].nunique()\n",
    "    ], axis=1)\n",
    "\n",
    "    out_df = pd.merge(input_df[agg_column], _agg_df, on=agg_column, how='left')\n",
    "    out_df = out_df.drop(columns=agg_column).add_suffix('_by_{}'.format(agg_column))\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四則演算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_operation_feature(input_df, is_test=False):\n",
    "    out_df = pd.DataFrame(index=input_df.index)\n",
    "    out_df['first_last_login_interval'] = input_df['first_login_interval'] - input_df['last_login_interval']\n",
    "    out_df['max_last_login_interval'] = input_df['max_login_interval'] - input_df['last_login_interval']\n",
    "    out_df['max_first_login_interval'] = input_df['max_login_interval'] - input_df['first_login_interval']\n",
    "    out_df['login_freq_frequency'] = input_df['login_frequency'] - input_df['frequency']\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 連続変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_features(input_df, is_test=False):\n",
    "    use_columns = [\n",
    "        # 連続変数\n",
    "        'first_login_interval',\n",
    "        'max_login_interval', \n",
    "        'frequency', \n",
    "        'login_frequency', \n",
    "        'last_login_interval',\n",
    "        'from_click',\n",
    "    ]\n",
    "    return input_df[use_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_features(input_df, is_test=False):\n",
    "    use_columns = [\n",
    "        'adnw_id',\n",
    "        'header_bidding',\n",
    "        'is_interstitial',\n",
    "        'pos'\n",
    "    ]\n",
    "    return input_df[use_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(input_df, is_test=False):\n",
    "    date_df = pd.DataFrame(pd.to_datetime(input_df['imp_at'], utc=True))\n",
    "    date_df['imp_at'] = date_df['imp_at'].dt.tz_convert('Asia/Tokyo')\n",
    "    date_df['day'] = date_df['imp_at'].dt.day\n",
    "    date_df['hour'] = date_df['imp_at'].dt.hour\n",
    "    date_df['total_minute'] = date_df['imp_at'].dt.hour*60+date_df['imp_at'].dt.minute\n",
    "    date_df['dayofweek'] = date_df['imp_at'].dt.dayofweek\n",
    "    date_df.drop(columns=['imp_at'], inplace=True)\n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_campaign_features(input_df, is_test=False):\n",
    "    campaign = pd.merge(input_df[['campaign_id']], campaign_df, left_on='campaign_id', right_on='id', how='left')\n",
    "    campaign.drop(columns=['campaign_id', 'id', 'mst_advertiser_id', 'mst_advertiser_order_id'], inplace=True)\n",
    "    return campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_game_feed_native_video_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gamefeed_features(input_df, is_test=False):\n",
    "    input_merge = pd.merge(input_df[['game_feed_id', 'advertiser_id', 'video_template_id']], map_gv_df, \n",
    "                           left_on='game_feed_id', right_on='mst_game_feed_id', how='left').drop(columns=['mst_game_feed_id'])\n",
    "    \n",
    "    horizontal = ad_video_df.copy()\n",
    "    left_keys = ['horizontal_mst_advertiser_video_id', 'advertiser_id']\n",
    "    right_keys = ['id', 'mst_advertiser_id']\n",
    "    horizontal.columns = [f'horizontal_{c}' if c not in right_keys else c for c in horizontal.columns]\n",
    "    input_merge = pd.merge(input_merge, horizontal, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys) \n",
    "    \n",
    "    vertical = ad_video_df.copy()\n",
    "    left_keys = ['vertical_mst_advertiser_video_id', 'advertiser_id']\n",
    "    right_keys = ['id', 'mst_advertiser_id']\n",
    "    vertical.columns = [f'vertical_{c}' if c not in right_keys else c for c in vertical.columns]\n",
    "    input_merge = pd.merge(input_merge, vertical, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys)\n",
    "    \n",
    "    left_keys = [\n",
    "        \"horizontal_mst_advertiser_video_id\",\n",
    "        \"game_feed_id\",\n",
    "        \"video_template_id\",\n",
    "    ]\n",
    "    right_keys = [\n",
    "        \"mst_advertiser_video_id\",\n",
    "        \"mst_game_feed_id\",\n",
    "        \"mst_video_template_id\",\n",
    "    ]\n",
    "    horizontal = ad_cvideo_df.copy()\n",
    "    horizontal.columns = [f\"horizontal_converted_{c}\" if c not in right_keys else c for c in horizontal.columns]\n",
    "    input_merge = pd.merge(input_merge, horizontal, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys) \n",
    "    \n",
    "    left_keys = [\n",
    "        \"vertical_mst_advertiser_video_id\",\n",
    "        \"game_feed_id\",\n",
    "        \"video_template_id\",\n",
    "    ]\n",
    "    right_keys = [\n",
    "        \"mst_advertiser_video_id\",\n",
    "        \"mst_game_feed_id\",\n",
    "        \"mst_video_template_id\",\n",
    "    ]\n",
    "    vertical = ad_cvideo_df.copy()\n",
    "    vertical.columns = [f\"vertical_converted_{c}\" if c not in right_keys else c for c in vertical.columns]\n",
    "    input_merge = pd.merge(input_merge, vertical, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys)\n",
    "    \n",
    "    input_merge.drop(columns=['game_feed_id', 'advertiser_id', 'video_template_id', \n",
    "                              'horizontal_mst_advertiser_video_id', 'vertical_mst_advertiser_video_id'], inplace=True)\n",
    "    \n",
    "    # merge\n",
    "    merge_col = ['duration', 'file_size', 'converted_file_size', 'converted_bitrate']\n",
    "    vert_horz = ['vertical_', 'horizontal_']\n",
    "    \n",
    "    for m_col in merge_col:\n",
    "        input_merge[m_col] = 0\n",
    "        for vh in vert_horz:\n",
    "            input_merge[m_col] = input_merge[m_col] + input_merge[vh+m_col].fillna(0)\n",
    "            input_merge.drop(columns=[vh+m_col], inplace=True)\n",
    "    \n",
    "    return input_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = [\n",
    "    create_continuous_features,\n",
    "    create_category_features,\n",
    "    create_label_encoding_features,\n",
    "    create_date_features,\n",
    "    create_campaign_features,\n",
    "    create_gamefeed_features,\n",
    "    create_targetencoding_features,\n",
    "    create_count_encoding_features,\n",
    "    create_onehot_encoding_features,\n",
    "    create_aggregate_features,\n",
    "    create_operation_feature\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature(input_df, is_test=False):\n",
    "    out_df = pd.DataFrame()\n",
    "    for func in processors:\n",
    "        _df = func(input_df, is_test)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train\n",
      "advertiser_id\n",
      "app_id\n",
      "media_app_id\n",
      "campaign_id\n",
      "for test\n"
     ]
    }
   ],
   "source": [
    "train_feat_df = to_feature(train_df)\n",
    "test_feat_df = to_feature(test_df, True)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_feat_df) == len(train_df)\n",
    "assert len(test_feat_df) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(train_feat_df.columns)\n",
    "print(feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.1,\n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv = list(fold.split(train_feat_df, y)) # もともとが generator なため明示的に list に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8263\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0961149\tvalid_0's pr_auc: 0.321652\n",
      "[200]\tvalid_0's binary_logloss: 0.0954468\tvalid_0's pr_auc: 0.327415\n",
      "[300]\tvalid_0's binary_logloss: 0.0954021\tvalid_0's pr_auc: 0.327017\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's binary_logloss: 0.0953872\tvalid_0's pr_auc: 0.327866\n",
      "Fold 0 PR-AUC: 0.3279\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8257\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0949951\tvalid_0's pr_auc: 0.32683\n",
      "[200]\tvalid_0's binary_logloss: 0.094517\tvalid_0's pr_auc: 0.32971\n",
      "[300]\tvalid_0's binary_logloss: 0.0944514\tvalid_0's pr_auc: 0.330252\n",
      "Early stopping, best iteration is:\n",
      "[280]\tvalid_0's binary_logloss: 0.0944192\tvalid_0's pr_auc: 0.330612\n",
      "Fold 1 PR-AUC: 0.3306\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8266\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0949775\tvalid_0's pr_auc: 0.325996\n",
      "[200]\tvalid_0's binary_logloss: 0.0943442\tvalid_0's pr_auc: 0.331395\n",
      "[300]\tvalid_0's binary_logloss: 0.094288\tvalid_0's pr_auc: 0.33255\n",
      "Early stopping, best iteration is:\n",
      "[249]\tvalid_0's binary_logloss: 0.0942751\tvalid_0's pr_auc: 0.332272\n",
      "Fold 2 PR-AUC: 0.3323\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8256\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0956919\tvalid_0's pr_auc: 0.32315\n",
      "[200]\tvalid_0's binary_logloss: 0.0951984\tvalid_0's pr_auc: 0.327907\n",
      "Early stopping, best iteration is:\n",
      "[189]\tvalid_0's binary_logloss: 0.0951947\tvalid_0's pr_auc: 0.328169\n",
      "Fold 3 PR-AUC: 0.3282\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8257\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0959919\tvalid_0's pr_auc: 0.318059\n",
      "[200]\tvalid_0's binary_logloss: 0.0954199\tvalid_0's pr_auc: 0.322859\n",
      "[300]\tvalid_0's binary_logloss: 0.0954298\tvalid_0's pr_auc: 0.323452\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid_0's binary_logloss: 0.0953698\tvalid_0's pr_auc: 0.323803\n",
      "Fold 4 PR-AUC: 0.3238\n",
      "FINISHED \\ whole score: 0.3284\n",
      "CPU times: user 4h 10min 47s, sys: 3min 30s, total: 4h 14min 18s\n",
      "Wall time: 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oof, models, score = train_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>imp_0</th>\n",
       "      <th>imp_1</th>\n",
       "      <th>imp_2</th>\n",
       "      <th>imp_3</th>\n",
       "      <th>imp_4</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>total_minute</td>\n",
       "      <td>1801</td>\n",
       "      <td>2035</td>\n",
       "      <td>2004</td>\n",
       "      <td>1618</td>\n",
       "      <td>2154</td>\n",
       "      <td>9612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>TGE_campaign_id</td>\n",
       "      <td>1573</td>\n",
       "      <td>1967</td>\n",
       "      <td>1784</td>\n",
       "      <td>1341</td>\n",
       "      <td>1850</td>\n",
       "      <td>8515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>CE_app_id</td>\n",
       "      <td>1591</td>\n",
       "      <td>1839</td>\n",
       "      <td>1748</td>\n",
       "      <td>1264</td>\n",
       "      <td>1785</td>\n",
       "      <td>8227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>TGE_app_id</td>\n",
       "      <td>1517</td>\n",
       "      <td>1954</td>\n",
       "      <td>1662</td>\n",
       "      <td>1277</td>\n",
       "      <td>1800</td>\n",
       "      <td>8210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>last_login_int_min_by_uid</td>\n",
       "      <td>1500</td>\n",
       "      <td>1916</td>\n",
       "      <td>1616</td>\n",
       "      <td>1230</td>\n",
       "      <td>1641</td>\n",
       "      <td>7903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OH_auction_type_id=4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH_user_type_id=4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vertical_converted_width</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vertical_converted_height</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vertical_converted_rectangle_type_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  imp_0  imp_1  imp_2  imp_3  imp_4  \\\n",
       "85                          total_minute   1801   2035   2004   1618   2154   \n",
       "83                       TGE_campaign_id   1573   1967   1784   1341   1850   \n",
       "84                             CE_app_id   1591   1839   1748   1264   1785   \n",
       "82                            TGE_app_id   1517   1954   1662   1277   1800   \n",
       "81             last_login_int_min_by_uid   1500   1916   1616   1230   1641   \n",
       "..                                   ...    ...    ...    ...    ...    ...   \n",
       "4                 OH_auction_type_id=4.0      2      3      3      1      3   \n",
       "3                      OH_user_type_id=4      1      3      1      1      2   \n",
       "2               vertical_converted_width      0      0      0      0      0   \n",
       "1              vertical_converted_height      0      0      0      0      0   \n",
       "0   vertical_converted_rectangle_type_id      0      0      0      0      0   \n",
       "\n",
       "     sum  \n",
       "85  9612  \n",
       "83  8515  \n",
       "84  8227  \n",
       "82  8210  \n",
       "81  7903  \n",
       "..   ...  \n",
       "4     12  \n",
       "3      8  \n",
       "2      0  \n",
       "1      0  \n",
       "0      0  \n",
       "\n",
       "[86 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_iteration 222\n",
      "best_iteration 280\n",
      "best_iteration 249\n",
      "best_iteration 189\n",
      "best_iteration 261\n",
      "CPU times: user 1min 55s, sys: 939 ms, total: 1min 56s\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_list = []\n",
    "for model in models:\n",
    "    print('best_iteration', model.best_iteration)\n",
    "    pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "    pred_list.append(pred)\n",
    "    \n",
    "pred = np.mean(pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "sub_df.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feature=86\n",
      "- score=0.3284\n"
     ]
    }
   ],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple_9: 8-1, count, one-hot, aggregate, operation\n",
    "- Wall time: 10min 48s\n",
    "- feature=86\n",
    "- score=0.3284\n",
    "- publicLB= 0.2313 ★best★\n",
    "\n",
    "#### simple_8-3: label->target : 'game_feed_id', 'game_template_id'\n",
    "- feature=46\n",
    "- score=0.2490\n",
    "- publicLB= 0.2183\n",
    "\n",
    "#### simple_8-2: label->target : category_id\n",
    "- feature=46\n",
    "- score=0.2492\n",
    "\n",
    "#### simple_8-1: label->target : campaign_id\n",
    "- feature=46\n",
    "- score=0.2495\n",
    "\n",
    "#### simple_7: 追加のtarget_encを抜いた\n",
    "- Wall time: 7min 21s (vCPU x 24、メモリ 96 GB)\n",
    "- feature=46\n",
    "- score=0.2486\n",
    "- publicLB= 0.2207\n",
    "\n",
    "#### simple_6: label_enc + target_enc\n",
    "- Wall time: 3min 56s (vCPU x 24、メモリ 96 GB)\n",
    "- feature=55\n",
    "- score=0.3255\n",
    "- publicLB= 0.1798 (Overfit)\n",
    "\n",
    "#### simple_5: target_enc (oof version)\n",
    "- Wall time: 7min 20s\n",
    "- feature=44\n",
    "- score=0.2383\n",
    "- publicLB= 0.2123\n",
    "\n",
    "#### simple_4: target_enc\n",
    "- Wall time: 8min\n",
    "- feature=44\n",
    "- score=0.2589\n",
    "- publicLB= 0.1973 (leak)\n",
    "\n",
    "#### simple_3: tuning\n",
    "- Wall time: 7min 19s\n",
    "- feature=41\n",
    "- score=0.2229\n",
    "- publicLB= 0.1970\n",
    "\n",
    "#### simple_2\n",
    "- Wall time: 47min 46s\n",
    "- feature= 41\n",
    "- score= 0.214588\n",
    "- publicLB= 0.1907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
