{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# import lightgbm as lgbm\n",
    "import optuna.integration.lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feather(filename):\n",
    "    df = pd.read_feather(os.path.join(INPUT_DIR, filename))\n",
    "    print(filename, df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.f (1997595, 35)\n",
      "test.f (390095, 30)\n"
     ]
    }
   ],
   "source": [
    "train_df = read_feather('train.f')\n",
    "test_df = read_feather('test.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaign.f (14627, 4)\n",
      "map_game_feed_native_video_assets.f (2796, 3)\n",
      "advertiser_video.f (11707, 6)\n",
      "advertiser_converted_video.f (198622, 8)\n"
     ]
    }
   ],
   "source": [
    "campaign_df = read_feather('campaign.f')\n",
    "map_gv_df = read_feather('map_game_feed_native_video_assets.f')\n",
    "ad_video_df = read_feather('advertiser_video.f')\n",
    "ad_cvideo_df = read_feather('advertiser_converted_video.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_cvideo_df (107493, 8)\n"
     ]
    }
   ],
   "source": [
    "ad_cvideo_df = ad_cvideo_df.drop_duplicates(\n",
    "    subset=['mst_advertiser_video_id', \n",
    "                   'mst_game_feed_id', \n",
    "                    'mst_video_template_id'], keep='last')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['vertical', 'horizontal'])\n",
    "ad_cvideo_df['rectangle_type_id'] = le.transform(ad_cvideo_df['rectangle_type'])\n",
    "ad_cvideo_df.drop(columns=['rectangle_type'], inplace=True)\n",
    "print('ad_cvideo_df', ad_cvideo_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 連続変数の特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_features(input_df):\n",
    "    use_columns = [\n",
    "        # 連続変数\n",
    "        'first_login_interval',\n",
    "        'max_login_interval', \n",
    "        'frequency', \n",
    "        'login_frequency', \n",
    "        'last_login_interval',\n",
    "        'from_click',\n",
    "    ]\n",
    "    return input_df[use_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category系の特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_features(input_df):\n",
    "    use_columns = [\n",
    "        # category 系の id. label-encoding として使う\n",
    "        'adnw_id',\n",
    "        'adspot_id',\n",
    "        'adspot_video_format_id',\n",
    "        'game_feed_asset_type_id',\n",
    "        'auction_type_id',\n",
    "        'category_id',\n",
    "        'header_bidding',\n",
    "        'is_interstitial',\n",
    "        'os',\n",
    "#  os_version',\n",
    "        'pos',\n",
    "        'user_type_id'\n",
    "    ]\n",
    "    return input_df[use_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### country_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_countrycode(input_df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(['None', 'JP', 'US', 'KR'])\n",
    "    return pd.DataFrame(le.transform(input_df['country_code'].fillna('None')), columns=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(input_df):\n",
    "    date_df = pd.DataFrame(pd.to_datetime(input_df['imp_at'], utc=True))\n",
    "    date_df['imp_at'] = date_df['imp_at'].dt.tz_convert('Asia/Tokyo')\n",
    "    date_df['day'] = date_df['imp_at'].dt.day\n",
    "    date_df['hour'] = date_df['imp_at'].dt.hour\n",
    "    date_df['total_minute'] = date_df['imp_at'].dt.hour*60+date_df['imp_at'].dt.minute\n",
    "    date_df['dayofweek'] = date_df['imp_at'].dt.dayofweek\n",
    "    date_df.drop(columns=['imp_at'], inplace=True)\n",
    "    return date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_campaign_features(input_df):\n",
    "    campaign = pd.merge(input_df[['campaign_id']], campaign_df, left_on='campaign_id', right_on='id', how='left')\n",
    "    campaign.drop(columns=['campaign_id', 'id', 'mst_advertiser_id', 'mst_advertiser_order_id'], inplace=True)\n",
    "    return campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_game_feed_native_video_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gamefeed_features(input_df):\n",
    "    input_merge = pd.merge(input_df[['game_feed_id', 'advertiser_id', 'video_template_id']], map_gv_df, \n",
    "                           left_on='game_feed_id', right_on='mst_game_feed_id', how='left').drop(columns=['mst_game_feed_id'])\n",
    "    \n",
    "    horizontal = ad_video_df.copy()\n",
    "    left_keys = ['horizontal_mst_advertiser_video_id', 'advertiser_id']\n",
    "    right_keys = ['id', 'mst_advertiser_id']\n",
    "    horizontal.columns = [f'horizontal_{c}' if c not in right_keys else c for c in horizontal.columns]\n",
    "    input_merge = pd.merge(input_merge, horizontal, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys) \n",
    "    \n",
    "    vertical = ad_video_df.copy()\n",
    "    left_keys = ['vertical_mst_advertiser_video_id', 'advertiser_id']\n",
    "    right_keys = ['id', 'mst_advertiser_id']\n",
    "    vertical.columns = [f'vertical_{c}' if c not in right_keys else c for c in vertical.columns]\n",
    "    input_merge = pd.merge(input_merge, vertical, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys)\n",
    "    \n",
    "    left_keys = [\n",
    "        \"horizontal_mst_advertiser_video_id\",\n",
    "        \"game_feed_id\",\n",
    "        \"video_template_id\",\n",
    "    ]\n",
    "    right_keys = [\n",
    "        \"mst_advertiser_video_id\",\n",
    "        \"mst_game_feed_id\",\n",
    "        \"mst_video_template_id\",\n",
    "    ]\n",
    "    horizontal = ad_cvideo_df.copy()\n",
    "    horizontal.columns = [f\"horizontal_converted_{c}\" if c not in right_keys else c for c in horizontal.columns]\n",
    "    input_merge = pd.merge(input_merge, horizontal, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys) \n",
    "    \n",
    "    left_keys = [\n",
    "        \"vertical_mst_advertiser_video_id\",\n",
    "        \"game_feed_id\",\n",
    "        \"video_template_id\",\n",
    "    ]\n",
    "    right_keys = [\n",
    "        \"mst_advertiser_video_id\",\n",
    "        \"mst_game_feed_id\",\n",
    "        \"mst_video_template_id\",\n",
    "    ]\n",
    "    vertical = ad_cvideo_df.copy()\n",
    "    vertical.columns = [f\"vertical_converted_{c}\" if c not in right_keys else c for c in vertical.columns]\n",
    "    input_merge = pd.merge(input_merge, vertical, left_on=left_keys, right_on=right_keys, how='left').drop(columns=right_keys)\n",
    "    \n",
    "    input_merge.drop(columns=['game_feed_id', 'advertiser_id', 'video_template_id', \n",
    "                              'horizontal_mst_advertiser_video_id', 'vertical_mst_advertiser_video_id'], inplace=True)\n",
    "    return input_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = [\n",
    "    create_continuous_features,\n",
    "    create_category_features,\n",
    "    create_countrycode,\n",
    "    create_date_features,\n",
    "    create_campaign_features,\n",
    "    create_gamefeed_features\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature(input_df):\n",
    "    out_df = pd.DataFrame()\n",
    "    for func in processors:\n",
    "        _df = func(input_df)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_df = to_feature(train_df)\n",
    "test_feat_df = to_feature(test_df)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_feat_df) == len(train_df)\n",
    "assert len(test_feat_df) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = len(train_feat_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv = list(fold.split(train_feat_df, y)) # もともとが generator なため明示的に list に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-15 15:19:48,617]\u001b[0m A new study created in memory with name: no-name-5ee51138-5f2f-4e99-b96e-0a86d0c365c3\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  14%|#4        | 1/7 [02:01<12:08, 121.34s/it]\u001b[32m[I 2020-11-15 15:21:49,957]\u001b[0m Trial 0 finished with value: 0.10884063224250155 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  14%|#4        | 1/7 [02:01<12:08, 121.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  29%|##8       | 2/7 [03:29<09:16, 111.33s/it]\u001b[32m[I 2020-11-15 15:23:17,940]\u001b[0m Trial 1 finished with value: 0.10896368191275302 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  29%|##8       | 2/7 [03:29<09:16, 111.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  43%|####2     | 3/7 [04:58<06:58, 104.65s/it]\u001b[32m[I 2020-11-15 15:24:46,998]\u001b[0m Trial 2 finished with value: 0.10906982907424413 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  43%|####2     | 3/7 [04:58<06:58, 104.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  57%|#####7    | 4/7 [06:46<05:16, 105.56s/it]\u001b[32m[I 2020-11-15 15:26:34,671]\u001b[0m Trial 3 finished with value: 0.10889813299098247 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  57%|#####7    | 4/7 [06:46<05:16, 105.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  71%|#######1  | 5/7 [08:01<03:13, 96.65s/it] \u001b[32m[I 2020-11-15 15:27:50,555]\u001b[0m Trial 4 finished with value: 0.1090050342141429 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  71%|#######1  | 5/7 [08:01<03:13, 96.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841:  86%|########5 | 6/7 [09:06<01:27, 87.09s/it]\u001b[32m[I 2020-11-15 15:28:55,341]\u001b[0m Trial 5 finished with value: 0.10928357410445236 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841:  86%|########5 | 6/7 [09:06<01:27, 87.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.108841: 100%|##########| 7/7 [10:08<00:00, 79.53s/it]\u001b[32m[I 2020-11-15 15:29:57,218]\u001b[0m Trial 6 finished with value: 0.10931850732774431 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.10884063224250155.\u001b[0m\n",
      "feature_fraction, val_score: 0.108841: 100%|##########| 7/7 [10:08<00:00, 86.94s/it]\n",
      "num_leaves, val_score: 0.108841:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:   5%|5         | 1/20 [00:42<13:20, 42.14s/it]\u001b[32m[I 2020-11-15 15:30:39,366]\u001b[0m Trial 7 finished with value: 0.10836800878177427 and parameters: {'num_leaves': 236}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:   5%|5         | 1/20 [00:42<13:20, 42.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  10%|#         | 2/20 [01:47<14:42, 49.01s/it]\u001b[32m[I 2020-11-15 15:31:44,408]\u001b[0m Trial 8 finished with value: 0.10889962046907202 and parameters: {'num_leaves': 61}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  10%|#         | 2/20 [01:47<14:42, 49.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  15%|#5        | 3/20 [02:29<13:17, 46.91s/it]\u001b[32m[I 2020-11-15 15:32:26,406]\u001b[0m Trial 9 finished with value: 0.1084047821483553 and parameters: {'num_leaves': 179}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  15%|#5        | 3/20 [02:29<13:17, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  20%|##        | 4/20 [03:07<11:48, 44.25s/it]\u001b[32m[I 2020-11-15 15:33:04,455]\u001b[0m Trial 10 finished with value: 0.1084235108872279 and parameters: {'num_leaves': 243}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  20%|##        | 4/20 [03:07<11:48, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  25%|##5       | 5/20 [03:45<10:35, 42.34s/it]\u001b[32m[I 2020-11-15 15:33:42,325]\u001b[0m Trial 11 finished with value: 0.10837881519882647 and parameters: {'num_leaves': 215}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  25%|##5       | 5/20 [03:45<10:35, 42.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  30%|###       | 6/20 [04:26<09:49, 42.08s/it]\u001b[32m[I 2020-11-15 15:34:23,793]\u001b[0m Trial 12 finished with value: 0.10840055266534117 and parameters: {'num_leaves': 256}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  30%|###       | 6/20 [04:26<09:49, 42.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108368:  35%|###5      | 7/20 [05:04<08:49, 40.69s/it]\u001b[32m[I 2020-11-15 15:35:01,263]\u001b[0m Trial 13 finished with value: 0.1084596480380913 and parameters: {'num_leaves': 184}. Best is trial 7 with value: 0.10836800878177427.\u001b[0m\n",
      "num_leaves, val_score: 0.108368:  35%|###5      | 7/20 [05:04<08:49, 40.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  40%|####      | 8/20 [05:44<08:06, 40.52s/it]\u001b[32m[I 2020-11-15 15:35:41,374]\u001b[0m Trial 14 finished with value: 0.10836487983490546 and parameters: {'num_leaves': 213}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  40%|####      | 8/20 [05:44<08:06, 40.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  45%|####5     | 9/20 [06:30<07:43, 42.12s/it]\u001b[32m[I 2020-11-15 15:36:27,226]\u001b[0m Trial 15 finished with value: 0.10851297835265393 and parameters: {'num_leaves': 120}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  45%|####5     | 9/20 [06:30<07:43, 42.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  50%|#####     | 10/20 [07:17<07:17, 43.79s/it]\u001b[32m[I 2020-11-15 15:37:14,923]\u001b[0m Trial 16 finished with value: 0.10846690119341607 and parameters: {'num_leaves': 147}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  50%|#####     | 10/20 [07:17<07:17, 43.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  55%|#####5    | 11/20 [07:56<06:20, 42.27s/it]\u001b[32m[I 2020-11-15 15:37:53,626]\u001b[0m Trial 17 finished with value: 0.10840932394698112 and parameters: {'num_leaves': 255}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  55%|#####5    | 11/20 [07:56<06:20, 42.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  60%|######    | 12/20 [08:38<05:36, 42.12s/it]\u001b[32m[I 2020-11-15 15:38:35,411]\u001b[0m Trial 18 finished with value: 0.10837487509074133 and parameters: {'num_leaves': 211}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  60%|######    | 12/20 [08:38<05:36, 42.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  65%|######5   | 13/20 [09:27<05:10, 44.38s/it]\u001b[32m[I 2020-11-15 15:39:25,077]\u001b[0m Trial 19 finished with value: 0.10876795113077965 and parameters: {'num_leaves': 92}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  65%|######5   | 13/20 [09:27<05:10, 44.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108365:  70%|#######   | 14/20 [10:06<04:15, 42.60s/it]\u001b[32m[I 2020-11-15 15:40:03,502]\u001b[0m Trial 20 finished with value: 0.10837881519882649 and parameters: {'num_leaves': 215}. Best is trial 14 with value: 0.10836487983490546.\u001b[0m\n",
      "num_leaves, val_score: 0.108365:  70%|#######   | 14/20 [10:06<04:15, 42.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332:  75%|#######5  | 15/20 [10:44<03:25, 41.16s/it]\u001b[32m[I 2020-11-15 15:40:41,312]\u001b[0m Trial 21 finished with value: 0.10833206010522908 and parameters: {'num_leaves': 212}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332:  75%|#######5  | 15/20 [10:44<03:25, 41.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332:  80%|########  | 16/20 [11:24<02:43, 40.81s/it]\u001b[32m[I 2020-11-15 15:41:21,312]\u001b[0m Trial 22 finished with value: 0.10843115959041695 and parameters: {'num_leaves': 177}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332:  80%|########  | 16/20 [11:24<02:43, 40.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332:  85%|########5 | 17/20 [12:02<02:00, 40.04s/it]\u001b[32m[I 2020-11-15 15:41:59,549]\u001b[0m Trial 23 finished with value: 0.10839777466525109 and parameters: {'num_leaves': 231}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332:  85%|########5 | 17/20 [12:02<02:00, 40.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332:  90%|######### | 18/20 [12:39<01:18, 39.31s/it]\u001b[32m[I 2020-11-15 15:42:37,142]\u001b[0m Trial 24 finished with value: 0.10839816245600614 and parameters: {'num_leaves': 196}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332:  90%|######### | 18/20 [12:39<01:18, 39.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332:  95%|#########5| 19/20 [13:27<00:41, 41.68s/it]\u001b[32m[I 2020-11-15 15:43:24,363]\u001b[0m Trial 25 finished with value: 0.10846690119341607 and parameters: {'num_leaves': 147}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332:  95%|#########5| 19/20 [13:27<00:41, 41.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.108332: 100%|##########| 20/20 [14:05<00:00, 40.81s/it]\u001b[32m[I 2020-11-15 15:44:03,130]\u001b[0m Trial 26 finished with value: 0.10836320625259825 and parameters: {'num_leaves': 253}. Best is trial 21 with value: 0.10833206010522908.\u001b[0m\n",
      "num_leaves, val_score: 0.108332: 100%|##########| 20/20 [14:05<00:00, 42.30s/it]\n",
      "bagging, val_score: 0.108332:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108320:  10%|#         | 1/10 [00:45<06:47, 45.22s/it]\u001b[32m[I 2020-11-15 15:44:48,358]\u001b[0m Trial 27 finished with value: 0.10831961174942051 and parameters: {'bagging_fraction': 0.9603647089936341, 'bagging_freq': 2}. Best is trial 27 with value: 0.10831961174942051.\u001b[0m\n",
      "bagging, val_score: 0.108320:  10%|#         | 1/10 [00:45<06:47, 45.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108320:  20%|##        | 2/10 [01:27<05:55, 44.48s/it]\u001b[32m[I 2020-11-15 15:45:31,093]\u001b[0m Trial 28 finished with value: 0.10833470438089653 and parameters: {'bagging_fraction': 0.9957020474040958, 'bagging_freq': 2}. Best is trial 27 with value: 0.10831961174942051.\u001b[0m\n",
      "bagging, val_score: 0.108320:  20%|##        | 2/10 [01:27<05:55, 44.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108320:  30%|###       | 3/10 [02:13<05:14, 44.92s/it]\u001b[32m[I 2020-11-15 15:46:17,040]\u001b[0m Trial 29 finished with value: 0.10834436556867914 and parameters: {'bagging_fraction': 0.9923505068723085, 'bagging_freq': 2}. Best is trial 27 with value: 0.10831961174942051.\u001b[0m\n",
      "bagging, val_score: 0.108320:  30%|###       | 3/10 [02:13<05:14, 44.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108320:  40%|####      | 4/10 [02:56<04:25, 44.23s/it]\u001b[32m[I 2020-11-15 15:46:59,669]\u001b[0m Trial 30 finished with value: 0.10839232708250224 and parameters: {'bagging_fraction': 0.9973009445509088, 'bagging_freq': 2}. Best is trial 27 with value: 0.10831961174942051.\u001b[0m\n",
      "bagging, val_score: 0.108320:  40%|####      | 4/10 [02:56<04:25, 44.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315:  50%|#####     | 5/10 [03:38<03:37, 43.56s/it]\u001b[32m[I 2020-11-15 15:47:41,677]\u001b[0m Trial 31 finished with value: 0.10831489858348657 and parameters: {'bagging_fraction': 0.9907178796872467, 'bagging_freq': 2}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315:  50%|#####     | 5/10 [03:38<03:37, 43.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315:  60%|######    | 6/10 [04:23<02:56, 44.08s/it]\u001b[32m[I 2020-11-15 15:48:26,951]\u001b[0m Trial 32 finished with value: 0.10832404116901027 and parameters: {'bagging_fraction': 0.9952667407888507, 'bagging_freq': 2}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315:  60%|######    | 6/10 [04:23<02:56, 44.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315:  70%|#######   | 7/10 [05:04<02:09, 43.12s/it]\u001b[32m[I 2020-11-15 15:49:07,849]\u001b[0m Trial 33 finished with value: 0.10861200430250233 and parameters: {'bagging_fraction': 0.7660883363989718, 'bagging_freq': 5}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315:  70%|#######   | 7/10 [05:04<02:09, 43.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315:  80%|########  | 8/10 [05:43<01:23, 41.85s/it]\u001b[32m[I 2020-11-15 15:49:46,737]\u001b[0m Trial 34 finished with value: 0.10853354161149514 and parameters: {'bagging_fraction': 0.8551177105177565, 'bagging_freq': 1}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315:  80%|########  | 8/10 [05:43<01:23, 41.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315:  90%|######### | 9/10 [06:19<00:39, 39.94s/it]\u001b[32m[I 2020-11-15 15:50:22,202]\u001b[0m Trial 35 finished with value: 0.1096773634667118 and parameters: {'bagging_fraction': 0.43587200626867945, 'bagging_freq': 3}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315:  90%|######### | 9/10 [06:19<00:39, 39.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.108315: 100%|##########| 10/10 [06:58<00:00, 39.92s/it]\u001b[32m[I 2020-11-15 15:51:02,091]\u001b[0m Trial 36 finished with value: 0.10846864279362962 and parameters: {'bagging_fraction': 0.8757596763983945, 'bagging_freq': 7}. Best is trial 31 with value: 0.10831489858348657.\u001b[0m\n",
      "bagging, val_score: 0.108315: 100%|##########| 10/10 [06:58<00:00, 41.90s/it]\n",
      "feature_fraction_stage2, val_score: 0.108315:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.108315:  33%|###3      | 1/3 [00:38<01:17, 38.86s/it]\u001b[32m[I 2020-11-15 15:51:40,956]\u001b[0m Trial 37 finished with value: 0.1087282416917308 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 0.1087282416917308.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.108315:  33%|###3      | 1/3 [00:38<01:17, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.108315:  67%|######6   | 2/3 [01:35<00:44, 44.23s/it]\u001b[32m[I 2020-11-15 15:52:37,709]\u001b[0m Trial 38 finished with value: 0.10849075580990233 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 38 with value: 0.10849075580990233.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.108315:  67%|######6   | 2/3 [01:35<00:44, 44.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.108315: 100%|##########| 3/3 [02:25<00:00, 46.02s/it]\u001b[32m[I 2020-11-15 15:53:27,900]\u001b[0m Trial 39 finished with value: 0.10837581696749339 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 39 with value: 0.10837581696749339.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.108315: 100%|##########| 3/3 [02:25<00:00, 48.60s/it]\n",
      "regularization_factors, val_score: 0.108315:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.108207:   5%|5         | 1/20 [00:42<13:35, 42.94s/it]\u001b[32m[I 2020-11-15 15:54:10,847]\u001b[0m Trial 40 finished with value: 0.1082069124098612 and parameters: {'lambda_l1': 4.0152301341547e-05, 'lambda_l2': 0.25017130099418683}. Best is trial 40 with value: 0.1082069124098612.\u001b[0m\n",
      "regularization_factors, val_score: 0.108207:   5%|5         | 1/20 [00:42<13:35, 42.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107808:  10%|#         | 2/20 [01:39<14:07, 47.09s/it]\u001b[32m[I 2020-11-15 15:55:07,602]\u001b[0m Trial 41 finished with value: 0.10780814269073244 and parameters: {'lambda_l1': 2.459187443737713e-05, 'lambda_l2': 1.3149695665065615}. Best is trial 41 with value: 0.10780814269073244.\u001b[0m\n",
      "regularization_factors, val_score: 0.107808:  10%|#         | 2/20 [01:39<14:07, 47.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107808:  15%|#5        | 3/20 [02:29<13:36, 48.04s/it]\u001b[32m[I 2020-11-15 15:55:57,866]\u001b[0m Trial 42 finished with value: 0.10791031140831576 and parameters: {'lambda_l1': 2.630569498709477e-05, 'lambda_l2': 1.005765340214611}. Best is trial 41 with value: 0.10780814269073244.\u001b[0m\n",
      "regularization_factors, val_score: 0.107808:  15%|#5        | 3/20 [02:29<13:36, 48.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107771:  20%|##        | 4/20 [03:41<14:40, 55.05s/it]\u001b[32m[I 2020-11-15 15:57:09,259]\u001b[0m Trial 43 finished with value: 0.10777061291165119 and parameters: {'lambda_l1': 3.872526084233591e-05, 'lambda_l2': 1.7882410541825418}. Best is trial 43 with value: 0.10777061291165119.\u001b[0m\n",
      "regularization_factors, val_score: 0.107771:  20%|##        | 4/20 [03:41<14:40, 55.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107771:  25%|##5       | 5/20 [04:42<14:11, 56.78s/it]\u001b[32m[I 2020-11-15 15:58:10,082]\u001b[0m Trial 44 finished with value: 0.10779738735201992 and parameters: {'lambda_l1': 3.962166326872457e-05, 'lambda_l2': 1.708415453776413}. Best is trial 43 with value: 0.10777061291165119.\u001b[0m\n",
      "regularization_factors, val_score: 0.107771:  25%|##5       | 5/20 [04:42<14:11, 56.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107771:  30%|###       | 6/20 [05:42<13:29, 57.85s/it]\u001b[32m[I 2020-11-15 15:59:10,423]\u001b[0m Trial 45 finished with value: 0.10780306168358028 and parameters: {'lambda_l1': 2.969285017267949e-05, 'lambda_l2': 1.5910847702055195}. Best is trial 43 with value: 0.10777061291165119.\u001b[0m\n",
      "regularization_factors, val_score: 0.107771:  30%|###       | 6/20 [05:42<13:29, 57.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  35%|###5      | 7/20 [07:05<14:09, 65.33s/it]\u001b[32m[I 2020-11-15 16:00:33,218]\u001b[0m Trial 46 finished with value: 0.1077258764303843 and parameters: {'lambda_l1': 2.7522712678013458e-05, 'lambda_l2': 1.759568963823917}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  35%|###5      | 7/20 [07:05<14:09, 65.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  40%|####      | 8/20 [08:07<12:52, 64.40s/it]\u001b[32m[I 2020-11-15 16:01:35,430]\u001b[0m Trial 47 finished with value: 0.10792545081200847 and parameters: {'lambda_l1': 3.415856902884164e-05, 'lambda_l2': 2.527083257723957}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  40%|####      | 8/20 [08:07<12:52, 64.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  45%|####5     | 9/20 [09:07<11:34, 63.17s/it]\u001b[32m[I 2020-11-15 16:02:35,736]\u001b[0m Trial 48 finished with value: 0.10788197204910518 and parameters: {'lambda_l1': 3.552605864136303e-05, 'lambda_l2': 2.287320541342843}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  45%|####5     | 9/20 [09:07<11:34, 63.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  50%|#####     | 10/20 [10:04<10:12, 61.24s/it]\u001b[32m[I 2020-11-15 16:03:32,474]\u001b[0m Trial 49 finished with value: 0.10789476214394983 and parameters: {'lambda_l1': 2.6787532964052956e-05, 'lambda_l2': 1.814523827214241}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  50%|#####     | 10/20 [10:04<10:12, 61.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  55%|#####5    | 11/20 [10:46<08:19, 55.55s/it]\u001b[32m[I 2020-11-15 16:04:14,742]\u001b[0m Trial 50 finished with value: 0.10831536238791686 and parameters: {'lambda_l1': 6.0938098710117e-08, 'lambda_l2': 4.222455725210905e-07}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  55%|#####5    | 11/20 [10:46<08:19, 55.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  60%|######    | 12/20 [11:49<07:41, 57.66s/it]\u001b[32m[I 2020-11-15 16:05:17,329]\u001b[0m Trial 51 finished with value: 0.10776636446349928 and parameters: {'lambda_l1': 3.911879249047325e-05, 'lambda_l2': 2.2993210902286445}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  60%|######    | 12/20 [11:49<07:41, 57.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  65%|######5   | 13/20 [12:40<06:30, 55.79s/it]\u001b[32m[I 2020-11-15 16:06:08,742]\u001b[0m Trial 52 finished with value: 0.10784007681853965 and parameters: {'lambda_l1': 3.546137592387273e-05, 'lambda_l2': 4.262839731383408}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  65%|######5   | 13/20 [12:40<06:30, 55.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107726:  70%|#######   | 14/20 [13:24<05:13, 52.21s/it]\u001b[32m[I 2020-11-15 16:06:52,618]\u001b[0m Trial 53 finished with value: 0.10846417305666063 and parameters: {'lambda_l1': 0.000589446748418633, 'lambda_l2': 0.023799813021111074}. Best is trial 46 with value: 0.1077258764303843.\u001b[0m\n",
      "regularization_factors, val_score: 0.107726:  70%|#######   | 14/20 [13:24<05:13, 52.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624:  75%|#######5  | 15/20 [14:47<05:06, 61.25s/it]\u001b[32m[I 2020-11-15 16:08:14,964]\u001b[0m Trial 54 finished with value: 0.10762392338783457 and parameters: {'lambda_l1': 6.419345380049121e-07, 'lambda_l2': 8.432801302426078}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624:  75%|#######5  | 15/20 [14:47<05:06, 61.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624:  80%|########  | 16/20 [15:30<03:43, 55.90s/it]\u001b[32m[I 2020-11-15 16:08:58,384]\u001b[0m Trial 55 finished with value: 0.10842263424546927 and parameters: {'lambda_l1': 2.2505398796172587e-07, 'lambda_l2': 0.03561142290431853}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624:  80%|########  | 16/20 [15:30<03:43, 55.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624:  85%|########5 | 17/20 [16:42<03:02, 60.78s/it]\u001b[32m[I 2020-11-15 16:10:10,535]\u001b[0m Trial 56 finished with value: 0.10767361464497754 and parameters: {'lambda_l1': 1.1632728982276379e-06, 'lambda_l2': 9.647119414648337}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624:  85%|########5 | 17/20 [16:42<03:02, 60.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624:  90%|######### | 18/20 [17:39<01:59, 59.52s/it]\u001b[32m[I 2020-11-15 16:11:07,107]\u001b[0m Trial 57 finished with value: 0.10776000340642172 and parameters: {'lambda_l1': 5.16448256056151e-07, 'lambda_l2': 7.9195104665729605}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624:  90%|######### | 18/20 [17:39<01:59, 59.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624:  95%|#########5| 19/20 [18:45<01:01, 61.47s/it]\u001b[32m[I 2020-11-15 16:12:13,152]\u001b[0m Trial 58 finished with value: 0.10767316029265367 and parameters: {'lambda_l1': 5.875791200273285e-07, 'lambda_l2': 9.465904293035546}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624:  95%|#########5| 19/20 [18:45<01:01, 61.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.107624: 100%|##########| 20/20 [19:46<00:00, 61.55s/it]\u001b[32m[I 2020-11-15 16:13:14,885]\u001b[0m Trial 59 finished with value: 0.10779734107162874 and parameters: {'lambda_l1': 4.2181274695494245e-07, 'lambda_l2': 8.05718000414992}. Best is trial 54 with value: 0.10762392338783457.\u001b[0m\n",
      "regularization_factors, val_score: 0.107624: 100%|##########| 20/20 [19:46<00:00, 59.35s/it]\n",
      "min_data_in_leaf, val_score: 0.107624:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.107624:  20%|##        | 1/5 [01:07<04:29, 67.46s/it]\u001b[32m[I 2020-11-15 16:14:22,352]\u001b[0m Trial 60 finished with value: 0.1076843212154368 and parameters: {'min_child_samples': 50}. Best is trial 60 with value: 0.1076843212154368.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.107624:  20%|##        | 1/5 [01:07<04:29, 67.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.107529:  40%|####      | 2/5 [02:23<03:30, 70.15s/it]\u001b[32m[I 2020-11-15 16:15:38,781]\u001b[0m Trial 61 finished with value: 0.10752893954228449 and parameters: {'min_child_samples': 100}. Best is trial 61 with value: 0.10752893954228449.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.107529:  40%|####      | 2/5 [02:23<03:30, 70.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.107529:  60%|######    | 3/5 [03:33<02:19, 69.97s/it]\u001b[32m[I 2020-11-15 16:16:48,322]\u001b[0m Trial 62 finished with value: 0.10764034451977081 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.10752893954228449.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.107529:  60%|######    | 3/5 [03:33<02:19, 69.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.107529:  80%|########  | 4/5 [04:55<01:13, 73.57s/it]\u001b[32m[I 2020-11-15 16:18:10,294]\u001b[0m Trial 63 finished with value: 0.1076631854372158 and parameters: {'min_child_samples': 5}. Best is trial 61 with value: 0.10752893954228449.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.107529:  80%|########  | 4/5 [04:55<01:13, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1863\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.107529: 100%|##########| 5/5 [06:18<00:00, 76.55s/it]\u001b[32m[I 2020-11-15 16:19:33,813]\u001b[0m Trial 64 finished with value: 0.10763984588945337 and parameters: {'min_child_samples': 10}. Best is trial 61 with value: 0.10752893954228449.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.107529: 100%|##########| 5/5 [06:18<00:00, 75.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'objective': 'binary', 'boosting_type': 'gbdt', 'seed': 0, 'learning_rate': 0.1, 'feature_pre_filter': False, 'lambda_l1': 6.419345380049121e-07, 'lambda_l2': 8.432801302426078, 'num_leaves': 212, 'feature_fraction': 0.4, 'bagging_fraction': 0.9907178796872467, 'bagging_freq': 2, 'min_child_samples': 100, 'num_iterations': 1000, 'early_stopping_round': 100}\n",
      "Best Iteration: 245\n",
      "Best Score: defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('binary_logloss', 0.10752893954228449), ('pr_auc', 0.22382995580267329)])})\n",
      "CPU times: user 6h 57min 57s, sys: 6min 31s, total: 7h 4min 28s\n",
      "Wall time: 59min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 36\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.111217\tvalid_0's pr_auc: 0.201038\n",
      "[200]\tvalid_0's binary_logloss: 0.109942\tvalid_0's pr_auc: 0.206783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's binary_logloss: 0.109278\tvalid_0's pr_auc: 0.210614\n",
      "[400]\tvalid_0's binary_logloss: 0.108899\tvalid_0's pr_auc: 0.21246\n",
      "[500]\tvalid_0's binary_logloss: 0.108661\tvalid_0's pr_auc: 0.213817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[600]\tvalid_0's binary_logloss: 0.108505\tvalid_0's pr_auc: 0.214109\n",
      "Early stopping, best iteration is:\n",
      "[595]\tvalid_0's binary_logloss: 0.108485\tvalid_0's pr_auc: 0.214655\n",
      "Fold 0 PR-AUC: 0.2147\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 36\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.110338\tvalid_0's pr_auc: 0.202071\n",
      "[200]\tvalid_0's binary_logloss: 0.108965\tvalid_0's pr_auc: 0.209112\n",
      "[300]\tvalid_0's binary_logloss: 0.108392\tvalid_0's pr_auc: 0.211279\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.107965\tvalid_0's pr_auc: 0.213774\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.107711\tvalid_0's pr_auc: 0.215661\n",
      "[600]\tvalid_0's binary_logloss: 0.107522\tvalid_0's pr_auc: 0.216839\n",
      "[700]\tvalid_0's binary_logloss: 0.107417\tvalid_0's pr_auc: 0.217531\n",
      "[800]\tvalid_0's binary_logloss: 0.107345\tvalid_0's pr_auc: 0.218139\n",
      "[900]\tvalid_0's binary_logloss: 0.107309\tvalid_0's pr_auc: 0.218235\n",
      "[1000]\tvalid_0's binary_logloss: 0.107286\tvalid_0's pr_auc: 0.218242\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[946]\tvalid_0's binary_logloss: 0.107265\tvalid_0's pr_auc: 0.218527\n",
      "Fold 1 PR-AUC: 0.2185\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1855\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 36\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.110374\tvalid_0's pr_auc: 0.202686\n",
      "[200]\tvalid_0's binary_logloss: 0.109065\tvalid_0's pr_auc: 0.209124\n",
      "[300]\tvalid_0's binary_logloss: 0.108495\tvalid_0's pr_auc: 0.211696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.108128\tvalid_0's pr_auc: 0.213366\n",
      "[500]\tvalid_0's binary_logloss: 0.107885\tvalid_0's pr_auc: 0.214776\n",
      "[600]\tvalid_0's binary_logloss: 0.107736\tvalid_0's pr_auc: 0.215357\n",
      "[700]\tvalid_0's binary_logloss: 0.107616\tvalid_0's pr_auc: 0.216035\n",
      "[800]\tvalid_0's binary_logloss: 0.107535\tvalid_0's pr_auc: 0.216331\n",
      "[900]\tvalid_0's binary_logloss: 0.10747\tvalid_0's pr_auc: 0.216888\n",
      "[1000]\tvalid_0's binary_logloss: 0.107417\tvalid_0's pr_auc: 0.217299\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.107417\tvalid_0's pr_auc: 0.217299\n",
      "Fold 2 PR-AUC: 0.2173\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 36\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.110733\tvalid_0's pr_auc: 0.201866\n",
      "[200]\tvalid_0's binary_logloss: 0.10941\tvalid_0's pr_auc: 0.208682\n",
      "[300]\tvalid_0's binary_logloss: 0.1088\tvalid_0's pr_auc: 0.212028\n",
      "[400]\tvalid_0's binary_logloss: 0.108379\tvalid_0's pr_auc: 0.214116\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.108202\tvalid_0's pr_auc: 0.215219\n",
      "[600]\tvalid_0's binary_logloss: 0.10803\tvalid_0's pr_auc: 0.215581\n",
      "[700]\tvalid_0's binary_logloss: 0.107922\tvalid_0's pr_auc: 0.216074\n",
      "[800]\tvalid_0's binary_logloss: 0.107819\tvalid_0's pr_auc: 0.216736\n",
      "[900]\tvalid_0's binary_logloss: 0.107755\tvalid_0's pr_auc: 0.217056\n",
      "[1000]\tvalid_0's binary_logloss: 0.107701\tvalid_0's pr_auc: 0.21757\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[988]\tvalid_0's binary_logloss: 0.107695\tvalid_0's pr_auc: 0.217573\n",
      "Fold 3 PR-AUC: 0.2176\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 1545684\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1855\n",
      "[LightGBM] [Info] Number of data points in the train set: 1598076, number of used features: 36\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032784 -> initscore=-3.384468\n",
      "[LightGBM] [Info] Start training from score -3.384468\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.111355\tvalid_0's pr_auc: 0.194843\n",
      "[200]\tvalid_0's binary_logloss: 0.110184\tvalid_0's pr_auc: 0.200396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's binary_logloss: 0.109556\tvalid_0's pr_auc: 0.203279\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.109172\tvalid_0's pr_auc: 0.204732\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's binary_logloss: 0.108885\tvalid_0's pr_auc: 0.205386\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[600]\tvalid_0's binary_logloss: 0.108763\tvalid_0's pr_auc: 0.206126\n",
      "[700]\tvalid_0's binary_logloss: 0.108554\tvalid_0's pr_auc: 0.206588\n",
      "[800]\tvalid_0's binary_logloss: 0.108462\tvalid_0's pr_auc: 0.207238\n",
      "[900]\tvalid_0's binary_logloss: 0.108382\tvalid_0's pr_auc: 0.207689\n",
      "[1000]\tvalid_0's binary_logloss: 0.10834\tvalid_0's pr_auc: 0.208322\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's binary_logloss: 0.108338\tvalid_0's pr_auc: 0.208325\n",
      "Fold 4 PR-AUC: 0.2083\n",
      "FINISHED \\ whole score: 0.2151\n",
      "CPU times: user 58min 43s, sys: 23.1 s, total: 59min 6s\n",
      "Wall time: 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oof, models, score = train_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>imp_0</th>\n",
       "      <th>imp_1</th>\n",
       "      <th>imp_2</th>\n",
       "      <th>imp_3</th>\n",
       "      <th>imp_4</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>max_login_interval</td>\n",
       "      <td>2070</td>\n",
       "      <td>3516</td>\n",
       "      <td>3641</td>\n",
       "      <td>3492</td>\n",
       "      <td>3616</td>\n",
       "      <td>16335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>last_login_interval</td>\n",
       "      <td>1976</td>\n",
       "      <td>3137</td>\n",
       "      <td>3358</td>\n",
       "      <td>3445</td>\n",
       "      <td>3460</td>\n",
       "      <td>15376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>first_login_interval</td>\n",
       "      <td>1745</td>\n",
       "      <td>3117</td>\n",
       "      <td>3174</td>\n",
       "      <td>3250</td>\n",
       "      <td>3109</td>\n",
       "      <td>14395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>horizontal_file_size</td>\n",
       "      <td>1922</td>\n",
       "      <td>2921</td>\n",
       "      <td>3096</td>\n",
       "      <td>2990</td>\n",
       "      <td>3136</td>\n",
       "      <td>14065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>total_minute</td>\n",
       "      <td>1344</td>\n",
       "      <td>2368</td>\n",
       "      <td>2594</td>\n",
       "      <td>2609</td>\n",
       "      <td>2574</td>\n",
       "      <td>11489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>horizontal_converted_file_size</td>\n",
       "      <td>1110</td>\n",
       "      <td>1881</td>\n",
       "      <td>2058</td>\n",
       "      <td>2045</td>\n",
       "      <td>2041</td>\n",
       "      <td>9135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>day</td>\n",
       "      <td>1175</td>\n",
       "      <td>1935</td>\n",
       "      <td>2005</td>\n",
       "      <td>1946</td>\n",
       "      <td>1948</td>\n",
       "      <td>9009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>horizontal_duration</td>\n",
       "      <td>1025</td>\n",
       "      <td>1250</td>\n",
       "      <td>1398</td>\n",
       "      <td>1356</td>\n",
       "      <td>1364</td>\n",
       "      <td>6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>login_frequency</td>\n",
       "      <td>670</td>\n",
       "      <td>1031</td>\n",
       "      <td>1108</td>\n",
       "      <td>1068</td>\n",
       "      <td>1147</td>\n",
       "      <td>5024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mst_user_type_id</td>\n",
       "      <td>812</td>\n",
       "      <td>1018</td>\n",
       "      <td>1031</td>\n",
       "      <td>1045</td>\n",
       "      <td>1035</td>\n",
       "      <td>4941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>frequency</td>\n",
       "      <td>457</td>\n",
       "      <td>773</td>\n",
       "      <td>871</td>\n",
       "      <td>831</td>\n",
       "      <td>840</td>\n",
       "      <td>3772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>adspot_id</td>\n",
       "      <td>433</td>\n",
       "      <td>664</td>\n",
       "      <td>757</td>\n",
       "      <td>753</td>\n",
       "      <td>775</td>\n",
       "      <td>3382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dayofweek</td>\n",
       "      <td>347</td>\n",
       "      <td>658</td>\n",
       "      <td>686</td>\n",
       "      <td>717</td>\n",
       "      <td>696</td>\n",
       "      <td>3104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>category_id</td>\n",
       "      <td>381</td>\n",
       "      <td>491</td>\n",
       "      <td>471</td>\n",
       "      <td>469</td>\n",
       "      <td>554</td>\n",
       "      <td>2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>adnw_id</td>\n",
       "      <td>250</td>\n",
       "      <td>415</td>\n",
       "      <td>441</td>\n",
       "      <td>403</td>\n",
       "      <td>390</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>horizontal_width</td>\n",
       "      <td>295</td>\n",
       "      <td>391</td>\n",
       "      <td>420</td>\n",
       "      <td>388</td>\n",
       "      <td>375</td>\n",
       "      <td>1869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>from_click</td>\n",
       "      <td>268</td>\n",
       "      <td>371</td>\n",
       "      <td>382</td>\n",
       "      <td>388</td>\n",
       "      <td>370</td>\n",
       "      <td>1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>os</td>\n",
       "      <td>242</td>\n",
       "      <td>364</td>\n",
       "      <td>392</td>\n",
       "      <td>327</td>\n",
       "      <td>375</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adspot_video_format_id</td>\n",
       "      <td>211</td>\n",
       "      <td>328</td>\n",
       "      <td>332</td>\n",
       "      <td>335</td>\n",
       "      <td>341</td>\n",
       "      <td>1547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>header_bidding</td>\n",
       "      <td>151</td>\n",
       "      <td>261</td>\n",
       "      <td>289</td>\n",
       "      <td>271</td>\n",
       "      <td>297</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hour</td>\n",
       "      <td>126</td>\n",
       "      <td>232</td>\n",
       "      <td>249</td>\n",
       "      <td>228</td>\n",
       "      <td>207</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>game_feed_asset_type_id</td>\n",
       "      <td>149</td>\n",
       "      <td>212</td>\n",
       "      <td>217</td>\n",
       "      <td>206</td>\n",
       "      <td>199</td>\n",
       "      <td>983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pos</td>\n",
       "      <td>134</td>\n",
       "      <td>213</td>\n",
       "      <td>173</td>\n",
       "      <td>205</td>\n",
       "      <td>195</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>auction_type_id</td>\n",
       "      <td>101</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>193</td>\n",
       "      <td>176</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>user_type_id</td>\n",
       "      <td>112</td>\n",
       "      <td>160</td>\n",
       "      <td>154</td>\n",
       "      <td>171</td>\n",
       "      <td>163</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is_interstitial</td>\n",
       "      <td>81</td>\n",
       "      <td>121</td>\n",
       "      <td>100</td>\n",
       "      <td>135</td>\n",
       "      <td>131</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>horizontal_converted_width</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>67</td>\n",
       "      <td>79</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>country</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>horizontal_converted_bitrate</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vertical_file_size</td>\n",
       "      <td>31</td>\n",
       "      <td>69</td>\n",
       "      <td>57</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vertical_duration</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>horizontal_converted_height</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>horizontal_converted_rectangle_type_id</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horizontal_height</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vertical_width</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vertical_converted_width</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vertical_height</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vertical_converted_bitrate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vertical_converted_file_size</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vertical_converted_height</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vertical_converted_rectangle_type_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name  imp_0  imp_1  imp_2  imp_3  imp_4  \\\n",
       "40                      max_login_interval   2070   3516   3641   3492   3616   \n",
       "39                     last_login_interval   1976   3137   3358   3445   3460   \n",
       "37                    first_login_interval   1745   3117   3174   3250   3109   \n",
       "38                    horizontal_file_size   1922   2921   3096   2990   3136   \n",
       "36                            total_minute   1344   2368   2594   2609   2574   \n",
       "34          horizontal_converted_file_size   1110   1881   2058   2045   2041   \n",
       "35                                     day   1175   1935   2005   1946   1948   \n",
       "33                     horizontal_duration   1025   1250   1398   1356   1364   \n",
       "31                         login_frequency    670   1031   1108   1068   1147   \n",
       "32                        mst_user_type_id    812   1018   1031   1045   1035   \n",
       "30                               frequency    457    773    871    831    840   \n",
       "29                               adspot_id    433    664    757    753    775   \n",
       "27                               dayofweek    347    658    686    717    696   \n",
       "28                             category_id    381    491    471    469    554   \n",
       "24                                 adnw_id    250    415    441    403    390   \n",
       "26                        horizontal_width    295    391    420    388    375   \n",
       "25                              from_click    268    371    382    388    370   \n",
       "23                                      os    242    364    392    327    375   \n",
       "22                  adspot_video_format_id    211    328    332    335    341   \n",
       "21                          header_bidding    151    261    289    271    297   \n",
       "18                                    hour    126    232    249    228    207   \n",
       "20                 game_feed_asset_type_id    149    212    217    206    199   \n",
       "19                                     pos    134    213    173    205    195   \n",
       "16                         auction_type_id    101    176    176    193    176   \n",
       "17                            user_type_id    112    160    154    171    163   \n",
       "15                         is_interstitial     81    121    100    135    131   \n",
       "13              horizontal_converted_width     55     80    100     67     79   \n",
       "14                                 country     66     75     80     73     75   \n",
       "12            horizontal_converted_bitrate     53     63     77     70     72   \n",
       "10                      vertical_file_size     31     69     57     55     64   \n",
       "11                       vertical_duration     34     44     49     52     50   \n",
       "9              horizontal_converted_height     13     24     52     47     42   \n",
       "6   horizontal_converted_rectangle_type_id      0      5      3      3      4   \n",
       "4                        horizontal_height      0      0      1      2      4   \n",
       "2                           vertical_width      0      0      0      0      0   \n",
       "3                 vertical_converted_width      0      0      0      0      0   \n",
       "1                          vertical_height      0      0      0      0      0   \n",
       "5               vertical_converted_bitrate      0      0      0      0      0   \n",
       "7             vertical_converted_file_size      0      0      0      0      0   \n",
       "8                vertical_converted_height      0      0      0      0      0   \n",
       "0     vertical_converted_rectangle_type_id      0      0      0      0      0   \n",
       "\n",
       "      sum  \n",
       "40  16335  \n",
       "39  15376  \n",
       "37  14395  \n",
       "38  14065  \n",
       "36  11489  \n",
       "34   9135  \n",
       "35   9009  \n",
       "33   6393  \n",
       "31   5024  \n",
       "32   4941  \n",
       "30   3772  \n",
       "29   3382  \n",
       "27   3104  \n",
       "28   2366  \n",
       "24   1899  \n",
       "26   1869  \n",
       "25   1779  \n",
       "23   1700  \n",
       "22   1547  \n",
       "21   1269  \n",
       "18   1042  \n",
       "20    983  \n",
       "19    920  \n",
       "16    822  \n",
       "17    760  \n",
       "15    568  \n",
       "13    381  \n",
       "14    369  \n",
       "12    335  \n",
       "10    276  \n",
       "11    229  \n",
       "9     178  \n",
       "6      15  \n",
       "4       7  \n",
       "2       0  \n",
       "3       0  \n",
       "1       0  \n",
       "5       0  \n",
       "7       0  \n",
       "8       0  \n",
       "0       0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_iteration 595\n",
      "best_iteration 946\n",
      "best_iteration 1000\n",
      "best_iteration 988\n",
      "best_iteration 997\n",
      "CPU times: user 3min 27s, sys: 430 ms, total: 3min 27s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_list = []\n",
    "for model in models:\n",
    "    print('best_iteration', model.best_iteration)\n",
    "    pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "    pred_list.append(pred)\n",
    "    \n",
    "pred = np.mean(pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-74e0595a9717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "sub_df.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple_2\n",
    "- Wall time: 47min 46s\n",
    "- feature= 41\n",
    "- score= 0.214588\n",
    "- publicLB= 0.1907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
