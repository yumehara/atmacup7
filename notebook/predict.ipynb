{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "# import optuna.integration.lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "pd.set_option('display.max_Columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_No = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_time_series = False\n",
    "is_subsample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (1997595, 35)\n",
      "test_df (390095, 30)\n",
      "train_feat_df (1997595, 78)\n",
      "test_feat_df (390095, 78)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.f')\n",
    "test_df = pd.read_feather('../input/test.f')\n",
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "\n",
    "train_feat_df = pd.read_feather('../input/train_feat_df_{}.f'.format(preprocess_No))\n",
    "test_feat_df = pd.read_feather('../input/test_feat_df_{}.f'.format(preprocess_No))\n",
    "print('train_feat_df', train_feat_df.shape)\n",
    "print('test_feat_df', test_feat_df.shape)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(train_feat_df.columns)\n",
    "print(feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.01,\n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if is_time_series:\n",
    "#     fold = TimeSeriesSplit(n_splits=5)\n",
    "# else:\n",
    "#     fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = list(fold.split(train_feat_df, y)) \n",
    "\n",
    "# tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lgbm(X, y, param):\n",
    "    if is_time_series:\n",
    "        fold = TimeSeriesSplit(n_splits=5)\n",
    "    else:\n",
    "        fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = list(fold.split(X, y)) \n",
    "\n",
    "    oof, models, score = train_lgbm(X, y, cv, params=param)\n",
    "    return oof, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred():\n",
    "    oof, models, score = kfold_lgbm(train_feat_df, y, lgbm_param)\n",
    "    pred_list = []\n",
    "    for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resampling_train_pred():\n",
    "    print(y.value_counts())\n",
    "    negative = y.value_counts()[0]\n",
    "    positive = y.value_counts()[1]\n",
    "    strategy = {0:int(negative*0.5), 1:int(positive)}\n",
    "\n",
    "    pred_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(3):\n",
    "        rus = RandomUnderSampler(random_state=i*9, sampling_strategy = strategy)\n",
    "        X_resampled, y_resampled = rus.fit_resample(train_feat_df, y)\n",
    "\n",
    "        lgbm_param['seed'] = i*9\n",
    "        oof, models, score = kfold_lgbm(X_resampled, y_resampled, lgbm_param)\n",
    "        score_list.append(score)\n",
    "\n",
    "        for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        print('----------------[{}] {:.4f}----------------'.format(i, score))\n",
    "\n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    score_ave = np.mean(score_list, axis=0)\n",
    "    return pred, score_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance(models).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1932105\n",
      "1      65490\n",
      "Name: target, dtype: int64\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6540\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174593\tvalid_0's pr_auc: 0.421162\n",
      "[200]\tvalid_0's binary_logloss: 0.160901\tvalid_0's pr_auc: 0.433443\n",
      "[300]\tvalid_0's binary_logloss: 0.155726\tvalid_0's pr_auc: 0.441277\n",
      "[400]\tvalid_0's binary_logloss: 0.153497\tvalid_0's pr_auc: 0.446154\n",
      "[500]\tvalid_0's binary_logloss: 0.152011\tvalid_0's pr_auc: 0.450033\n",
      "[600]\tvalid_0's binary_logloss: 0.151091\tvalid_0's pr_auc: 0.453148\n",
      "[700]\tvalid_0's binary_logloss: 0.150488\tvalid_0's pr_auc: 0.455512\n",
      "[800]\tvalid_0's binary_logloss: 0.150013\tvalid_0's pr_auc: 0.457406\n",
      "[900]\tvalid_0's binary_logloss: 0.149681\tvalid_0's pr_auc: 0.458869\n",
      "[1000]\tvalid_0's binary_logloss: 0.149429\tvalid_0's pr_auc: 0.459866\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.149429\tvalid_0's pr_auc: 0.459866\n",
      "Fold 0 PR-AUC: 0.4599\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6564\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.1742\tvalid_0's pr_auc: 0.428617\n",
      "[200]\tvalid_0's binary_logloss: 0.160147\tvalid_0's pr_auc: 0.443026\n",
      "[300]\tvalid_0's binary_logloss: 0.154767\tvalid_0's pr_auc: 0.451727\n",
      "[400]\tvalid_0's binary_logloss: 0.152405\tvalid_0's pr_auc: 0.457274\n",
      "[500]\tvalid_0's binary_logloss: 0.150816\tvalid_0's pr_auc: 0.461529\n",
      "[600]\tvalid_0's binary_logloss: 0.149832\tvalid_0's pr_auc: 0.464711\n",
      "[700]\tvalid_0's binary_logloss: 0.149175\tvalid_0's pr_auc: 0.46716\n",
      "[800]\tvalid_0's binary_logloss: 0.148692\tvalid_0's pr_auc: 0.468864\n",
      "[900]\tvalid_0's binary_logloss: 0.148338\tvalid_0's pr_auc: 0.47021\n",
      "[1000]\tvalid_0's binary_logloss: 0.148098\tvalid_0's pr_auc: 0.471176\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148098\tvalid_0's pr_auc: 0.471176\n",
      "Fold 1 PR-AUC: 0.4712\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6567\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174935\tvalid_0's pr_auc: 0.417321\n",
      "[200]\tvalid_0's binary_logloss: 0.161234\tvalid_0's pr_auc: 0.430143\n",
      "[300]\tvalid_0's binary_logloss: 0.155977\tvalid_0's pr_auc: 0.438573\n",
      "[400]\tvalid_0's binary_logloss: 0.153705\tvalid_0's pr_auc: 0.44361\n",
      "[500]\tvalid_0's binary_logloss: 0.152184\tvalid_0's pr_auc: 0.447831\n",
      "[600]\tvalid_0's binary_logloss: 0.151203\tvalid_0's pr_auc: 0.451229\n",
      "[700]\tvalid_0's binary_logloss: 0.150544\tvalid_0's pr_auc: 0.453971\n",
      "[800]\tvalid_0's binary_logloss: 0.150061\tvalid_0's pr_auc: 0.455753\n",
      "[900]\tvalid_0's binary_logloss: 0.14968\tvalid_0's pr_auc: 0.457464\n",
      "[1000]\tvalid_0's binary_logloss: 0.149428\tvalid_0's pr_auc: 0.45847\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.149428\tvalid_0's pr_auc: 0.45847\n",
      "Fold 2 PR-AUC: 0.4585\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6561\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174074\tvalid_0's pr_auc: 0.427166\n",
      "[200]\tvalid_0's binary_logloss: 0.160038\tvalid_0's pr_auc: 0.440953\n",
      "[300]\tvalid_0's binary_logloss: 0.154651\tvalid_0's pr_auc: 0.449541\n",
      "[400]\tvalid_0's binary_logloss: 0.152279\tvalid_0's pr_auc: 0.455098\n",
      "[500]\tvalid_0's binary_logloss: 0.150661\tvalid_0's pr_auc: 0.459511\n",
      "[600]\tvalid_0's binary_logloss: 0.149671\tvalid_0's pr_auc: 0.462957\n",
      "[700]\tvalid_0's binary_logloss: 0.149019\tvalid_0's pr_auc: 0.465395\n",
      "[800]\tvalid_0's binary_logloss: 0.14852\tvalid_0's pr_auc: 0.467316\n",
      "[900]\tvalid_0's binary_logloss: 0.148135\tvalid_0's pr_auc: 0.469064\n",
      "[1000]\tvalid_0's binary_logloss: 0.147853\tvalid_0's pr_auc: 0.470324\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.147853\tvalid_0's pr_auc: 0.470324\n",
      "Fold 3 PR-AUC: 0.4703\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6564\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174445\tvalid_0's pr_auc: 0.426871\n",
      "[200]\tvalid_0's binary_logloss: 0.160627\tvalid_0's pr_auc: 0.439042\n",
      "[300]\tvalid_0's binary_logloss: 0.155408\tvalid_0's pr_auc: 0.447049\n",
      "[400]\tvalid_0's binary_logloss: 0.153132\tvalid_0's pr_auc: 0.452249\n",
      "[500]\tvalid_0's binary_logloss: 0.151608\tvalid_0's pr_auc: 0.456191\n",
      "[600]\tvalid_0's binary_logloss: 0.150641\tvalid_0's pr_auc: 0.459423\n",
      "[700]\tvalid_0's binary_logloss: 0.150015\tvalid_0's pr_auc: 0.461792\n",
      "[800]\tvalid_0's binary_logloss: 0.149531\tvalid_0's pr_auc: 0.463644\n",
      "[900]\tvalid_0's binary_logloss: 0.149188\tvalid_0's pr_auc: 0.46515\n",
      "[1000]\tvalid_0's binary_logloss: 0.148918\tvalid_0's pr_auc: 0.466219\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148918\tvalid_0's pr_auc: 0.466219\n",
      "Fold 4 PR-AUC: 0.4662\n",
      "FINISHED \\ whole score: 0.4651\n",
      "----------------[0] 0.4651----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6587\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174034\tvalid_0's pr_auc: 0.419166\n",
      "[200]\tvalid_0's binary_logloss: 0.161204\tvalid_0's pr_auc: 0.431081\n",
      "[300]\tvalid_0's binary_logloss: 0.156161\tvalid_0's pr_auc: 0.439454\n",
      "[400]\tvalid_0's binary_logloss: 0.153658\tvalid_0's pr_auc: 0.445013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's binary_logloss: 0.152325\tvalid_0's pr_auc: 0.448884\n",
      "[600]\tvalid_0's binary_logloss: 0.151451\tvalid_0's pr_auc: 0.451772\n",
      "[700]\tvalid_0's binary_logloss: 0.150787\tvalid_0's pr_auc: 0.454251\n",
      "[800]\tvalid_0's binary_logloss: 0.15033\tvalid_0's pr_auc: 0.456228\n",
      "[900]\tvalid_0's binary_logloss: 0.149949\tvalid_0's pr_auc: 0.457887\n",
      "[1000]\tvalid_0's binary_logloss: 0.149638\tvalid_0's pr_auc: 0.459153\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.149638\tvalid_0's pr_auc: 0.459153\n",
      "Fold 0 PR-AUC: 0.4592\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6553\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173563\tvalid_0's pr_auc: 0.427334\n",
      "[200]\tvalid_0's binary_logloss: 0.1604\tvalid_0's pr_auc: 0.439328\n",
      "[300]\tvalid_0's binary_logloss: 0.155168\tvalid_0's pr_auc: 0.44776\n",
      "[400]\tvalid_0's binary_logloss: 0.152595\tvalid_0's pr_auc: 0.453486\n",
      "[500]\tvalid_0's binary_logloss: 0.151229\tvalid_0's pr_auc: 0.457367\n",
      "[600]\tvalid_0's binary_logloss: 0.150332\tvalid_0's pr_auc: 0.460372\n",
      "[700]\tvalid_0's binary_logloss: 0.149657\tvalid_0's pr_auc: 0.462794\n",
      "[800]\tvalid_0's binary_logloss: 0.149219\tvalid_0's pr_auc: 0.464612\n",
      "[900]\tvalid_0's binary_logloss: 0.14885\tvalid_0's pr_auc: 0.466072\n",
      "[1000]\tvalid_0's binary_logloss: 0.148581\tvalid_0's pr_auc: 0.467122\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148581\tvalid_0's pr_auc: 0.467122\n",
      "Fold 1 PR-AUC: 0.4671\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6558\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173422\tvalid_0's pr_auc: 0.425033\n",
      "[200]\tvalid_0's binary_logloss: 0.160276\tvalid_0's pr_auc: 0.436109\n",
      "[300]\tvalid_0's binary_logloss: 0.155062\tvalid_0's pr_auc: 0.444349\n",
      "[400]\tvalid_0's binary_logloss: 0.152507\tvalid_0's pr_auc: 0.449895\n",
      "[500]\tvalid_0's binary_logloss: 0.151158\tvalid_0's pr_auc: 0.453882\n",
      "[600]\tvalid_0's binary_logloss: 0.150282\tvalid_0's pr_auc: 0.456768\n",
      "[700]\tvalid_0's binary_logloss: 0.149639\tvalid_0's pr_auc: 0.459145\n",
      "[800]\tvalid_0's binary_logloss: 0.149212\tvalid_0's pr_auc: 0.460907\n",
      "[900]\tvalid_0's binary_logloss: 0.148851\tvalid_0's pr_auc: 0.462401\n",
      "[1000]\tvalid_0's binary_logloss: 0.148571\tvalid_0's pr_auc: 0.463499\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148571\tvalid_0's pr_auc: 0.463499\n",
      "Fold 2 PR-AUC: 0.4635\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6555\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173665\tvalid_0's pr_auc: 0.425195\n",
      "[200]\tvalid_0's binary_logloss: 0.160544\tvalid_0's pr_auc: 0.436402\n",
      "[300]\tvalid_0's binary_logloss: 0.155458\tvalid_0's pr_auc: 0.444461\n",
      "[400]\tvalid_0's binary_logloss: 0.152909\tvalid_0's pr_auc: 0.450147\n",
      "[500]\tvalid_0's binary_logloss: 0.151565\tvalid_0's pr_auc: 0.453936\n",
      "[600]\tvalid_0's binary_logloss: 0.150705\tvalid_0's pr_auc: 0.456801\n",
      "[700]\tvalid_0's binary_logloss: 0.150045\tvalid_0's pr_auc: 0.459205\n",
      "[800]\tvalid_0's binary_logloss: 0.14959\tvalid_0's pr_auc: 0.461023\n",
      "[900]\tvalid_0's binary_logloss: 0.14922\tvalid_0's pr_auc: 0.46259\n",
      "[1000]\tvalid_0's binary_logloss: 0.148952\tvalid_0's pr_auc: 0.463801\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148952\tvalid_0's pr_auc: 0.463801\n",
      "Fold 3 PR-AUC: 0.4638\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6584\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173218\tvalid_0's pr_auc: 0.427609\n",
      "[200]\tvalid_0's binary_logloss: 0.16\tvalid_0's pr_auc: 0.439741\n",
      "[300]\tvalid_0's binary_logloss: 0.154825\tvalid_0's pr_auc: 0.448329\n",
      "[400]\tvalid_0's binary_logloss: 0.152261\tvalid_0's pr_auc: 0.454225\n",
      "[500]\tvalid_0's binary_logloss: 0.150905\tvalid_0's pr_auc: 0.458271\n",
      "[600]\tvalid_0's binary_logloss: 0.150014\tvalid_0's pr_auc: 0.461306\n",
      "[700]\tvalid_0's binary_logloss: 0.149346\tvalid_0's pr_auc: 0.463711\n",
      "[800]\tvalid_0's binary_logloss: 0.148885\tvalid_0's pr_auc: 0.465548\n",
      "[900]\tvalid_0's binary_logloss: 0.14852\tvalid_0's pr_auc: 0.46711\n",
      "[1000]\tvalid_0's binary_logloss: 0.148263\tvalid_0's pr_auc: 0.468173\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.148263\tvalid_0's pr_auc: 0.468173\n",
      "Fold 4 PR-AUC: 0.4682\n",
      "FINISHED \\ whole score: 0.4643\n",
      "----------------[1] 0.4643----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6561\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174324\tvalid_0's pr_auc: 0.420335\n",
      "[200]\tvalid_0's binary_logloss: 0.160706\tvalid_0's pr_auc: 0.433329\n",
      "[300]\tvalid_0's binary_logloss: 0.155623\tvalid_0's pr_auc: 0.440842\n",
      "[400]\tvalid_0's binary_logloss: 0.153094\tvalid_0's pr_auc: 0.446706\n",
      "[500]\tvalid_0's binary_logloss: 0.151638\tvalid_0's pr_auc: 0.4512\n",
      "[600]\tvalid_0's binary_logloss: 0.150744\tvalid_0's pr_auc: 0.45465\n",
      "[700]\tvalid_0's binary_logloss: 0.15008\tvalid_0's pr_auc: 0.457288\n",
      "[800]\tvalid_0's binary_logloss: 0.149634\tvalid_0's pr_auc: 0.459108\n",
      "[900]\tvalid_0's binary_logloss: 0.149303\tvalid_0's pr_auc: 0.460721\n",
      "[1000]\tvalid_0's binary_logloss: 0.14905\tvalid_0's pr_auc: 0.462019\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.14905\tvalid_0's pr_auc: 0.462019\n",
      "Fold 0 PR-AUC: 0.4620\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6563\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174305\tvalid_0's pr_auc: 0.422243\n",
      "[200]\tvalid_0's binary_logloss: 0.160879\tvalid_0's pr_auc: 0.434015\n",
      "[300]\tvalid_0's binary_logloss: 0.155906\tvalid_0's pr_auc: 0.44147\n",
      "[400]\tvalid_0's binary_logloss: 0.153487\tvalid_0's pr_auc: 0.446983\n",
      "[500]\tvalid_0's binary_logloss: 0.152075\tvalid_0's pr_auc: 0.451318\n",
      "[600]\tvalid_0's binary_logloss: 0.15122\tvalid_0's pr_auc: 0.454464\n",
      "[700]\tvalid_0's binary_logloss: 0.150565\tvalid_0's pr_auc: 0.45695\n",
      "[800]\tvalid_0's binary_logloss: 0.150141\tvalid_0's pr_auc: 0.458711\n",
      "[900]\tvalid_0's binary_logloss: 0.149789\tvalid_0's pr_auc: 0.460352\n",
      "[1000]\tvalid_0's binary_logloss: 0.149547\tvalid_0's pr_auc: 0.461307\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.149547\tvalid_0's pr_auc: 0.461311\n",
      "Fold 1 PR-AUC: 0.4613\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6565\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.174287\tvalid_0's pr_auc: 0.422807\n",
      "[200]\tvalid_0's binary_logloss: 0.160891\tvalid_0's pr_auc: 0.434508\n",
      "[300]\tvalid_0's binary_logloss: 0.155968\tvalid_0's pr_auc: 0.441718\n",
      "[400]\tvalid_0's binary_logloss: 0.153551\tvalid_0's pr_auc: 0.447247\n",
      "[500]\tvalid_0's binary_logloss: 0.15213\tvalid_0's pr_auc: 0.451427\n",
      "[600]\tvalid_0's binary_logloss: 0.151285\tvalid_0's pr_auc: 0.454468\n",
      "[700]\tvalid_0's binary_logloss: 0.150637\tvalid_0's pr_auc: 0.456851\n",
      "[800]\tvalid_0's binary_logloss: 0.150187\tvalid_0's pr_auc: 0.458524\n",
      "[900]\tvalid_0's binary_logloss: 0.149862\tvalid_0's pr_auc: 0.45997\n",
      "[1000]\tvalid_0's binary_logloss: 0.149579\tvalid_0's pr_auc: 0.461362\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.149579\tvalid_0's pr_auc: 0.461362\n",
      "Fold 2 PR-AUC: 0.4614\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6576\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173288\tvalid_0's pr_auc: 0.432476\n",
      "[200]\tvalid_0's binary_logloss: 0.159476\tvalid_0's pr_auc: 0.445775\n",
      "[300]\tvalid_0's binary_logloss: 0.154266\tvalid_0's pr_auc: 0.453932\n",
      "[400]\tvalid_0's binary_logloss: 0.151668\tvalid_0's pr_auc: 0.459858\n",
      "[500]\tvalid_0's binary_logloss: 0.150124\tvalid_0's pr_auc: 0.464275\n",
      "[600]\tvalid_0's binary_logloss: 0.149205\tvalid_0's pr_auc: 0.467396\n",
      "[700]\tvalid_0's binary_logloss: 0.148519\tvalid_0's pr_auc: 0.469693\n",
      "[800]\tvalid_0's binary_logloss: 0.148053\tvalid_0's pr_auc: 0.471422\n",
      "[900]\tvalid_0's binary_logloss: 0.147654\tvalid_0's pr_auc: 0.473252\n",
      "[1000]\tvalid_0's binary_logloss: 0.147361\tvalid_0's pr_auc: 0.47451\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.147361\tvalid_0's pr_auc: 0.47451\n",
      "Fold 3 PR-AUC: 0.4745\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6565\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.173415\tvalid_0's pr_auc: 0.429033\n",
      "[200]\tvalid_0's binary_logloss: 0.159648\tvalid_0's pr_auc: 0.441422\n",
      "[300]\tvalid_0's binary_logloss: 0.154507\tvalid_0's pr_auc: 0.449071\n",
      "[400]\tvalid_0's binary_logloss: 0.15205\tvalid_0's pr_auc: 0.454159\n",
      "[500]\tvalid_0's binary_logloss: 0.150594\tvalid_0's pr_auc: 0.457868\n",
      "[600]\tvalid_0's binary_logloss: 0.149727\tvalid_0's pr_auc: 0.460651\n",
      "[700]\tvalid_0's binary_logloss: 0.149075\tvalid_0's pr_auc: 0.463059\n",
      "[800]\tvalid_0's binary_logloss: 0.148631\tvalid_0's pr_auc: 0.464667\n",
      "[900]\tvalid_0's binary_logloss: 0.148285\tvalid_0's pr_auc: 0.466128\n",
      "[1000]\tvalid_0's binary_logloss: 0.14805\tvalid_0's pr_auc: 0.467175\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.14805\tvalid_0's pr_auc: 0.46718\n",
      "Fold 4 PR-AUC: 0.4672\n",
      "FINISHED \\ whole score: 0.4651\n",
      "----------------[2] 0.4651----------------\n",
      "CPU times: user 17h 51min 20s, sys: 34min 40s, total: 18h 26min 1s\n",
      "Wall time: 46min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_subsample:\n",
    "    pred, score = resampling_train_pred()\n",
    "else:\n",
    "    pred, score = train_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/submission_sub.csv\n"
     ]
    }
   ],
   "source": [
    "out_filename = 'submission'\n",
    "if is_time_series:\n",
    "    out_filename = out_filename + '_ts'\n",
    "\n",
    "if is_subsample:\n",
    "    out_filename = out_filename + '_sub'\n",
    "\n",
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "filepath = os.path.join(OUTPUT_DIR, out_filename + '.csv')\n",
    "sub_df.to_csv(filepath, index=False)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feature=78\n",
      "- score=0.4648\n"
     ]
    }
   ],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('../output/submission.csv')\n",
    "# sub_ts = pd.read_csv('../output/submission_ts.csv')\n",
    "# assert len(sub) == len(sub_ts)\n",
    "# sub['target'] = (sub['target'] + sub_ts['target'])/2\n",
    "# sub.to_csv('../output/ensumble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsampling_32: learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.4648\n",
    "- publicLB= 0.2415\n",
    "- privateLB= 0.2587\n",
    "\n",
    "#### subsampling_31: learning_rate:0.1\n",
    "- feature=78\n",
    "- score=0.4638\n",
    "- publicLB= 0.2430\n",
    "- privateLB= 0.2588\n",
    "\n",
    "#### ensumble_30: simple_30+ts_30\n",
    "- publicLB= 0.2393\n",
    "- privateLB= 0.2560\n",
    "\n",
    "#### ts_30: learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.2290\n",
    "- publicLB= 0.2308\n",
    "- privateLB= 0.2463\n",
    "\n",
    "#### simple_30: simple_27相当 learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.3277\n",
    "- publicLB= 0.2430 ★best★\n",
    "- privateLB= 0.2599\n",
    "\n",
    "#### subsampling_29: \n",
    "- feature=78\n",
    "- score=0.6501\n",
    "- publicLB= 0.2398\n",
    "- privateLB= 0.2576\n",
    "\n",
    "#### simple_27: pivot('dayofweek', 'hour_zone')\n",
    "- feature=78\n",
    "- score=0.3273\n",
    "- publicLB= 0.2420\n",
    "- privateLB= 0.2582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
