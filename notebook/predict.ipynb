{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "# import optuna.integration.lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "pd.set_option('display.max_Columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_No = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_time_series = False\n",
    "is_subsample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (1997595, 35)\n",
      "test_df (390095, 30)\n",
      "train_feat_df (1997595, 78)\n",
      "test_feat_df (390095, 78)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.f')\n",
    "test_df = pd.read_feather('../input/test.f')\n",
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "\n",
    "train_feat_df = pd.read_feather('../input/train_feat_df_{}.f'.format(preprocess_No))\n",
    "test_feat_df = pd.read_feather('../input/test_feat_df_{}.f'.format(preprocess_No))\n",
    "print('train_feat_df', train_feat_df.shape)\n",
    "print('test_feat_df', test_feat_df.shape)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(train_feat_df.columns)\n",
    "print(feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.1,\n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if is_time_series:\n",
    "#     fold = TimeSeriesSplit(n_splits=5)\n",
    "# else:\n",
    "#     fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = list(fold.split(train_feat_df, y)) \n",
    "\n",
    "# tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lgbm(X, y, param):\n",
    "    if is_time_series:\n",
    "        fold = TimeSeriesSplit(n_splits=5)\n",
    "    else:\n",
    "        fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = list(fold.split(X, y)) \n",
    "\n",
    "    oof, models, score = train_lgbm(X, y, cv, params=param)\n",
    "    return oof, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred():\n",
    "    oof, models, score = kfold_lgbm(train_feat_df, y, lgbm_param)\n",
    "    pred_list = []\n",
    "    for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resampling_train_pred():\n",
    "    print(y.value_counts())\n",
    "    negative = y.value_counts()[0]\n",
    "    positive = y.value_counts()[1]\n",
    "    strategy = {0:int(negative*0.5), 1:int(positive)}\n",
    "\n",
    "    pred_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(3):\n",
    "        rus = RandomUnderSampler(random_state=i*9, sampling_strategy = strategy)\n",
    "        X_resampled, y_resampled = rus.fit_resample(train_feat_df, y)\n",
    "\n",
    "        lgbm_param['seed'] = i*9\n",
    "        oof, models, score = kfold_lgbm(X_resampled, y_resampled, lgbm_param)\n",
    "        score_list.append(score)\n",
    "\n",
    "        for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        print('----------------[{}] {:.4f}----------------'.format(i, score))\n",
    "\n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    score_ave = np.mean(score_list, axis=0)\n",
    "    return pred, score_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance(models).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1932105\n",
      "1      65490\n",
      "Name: target, dtype: int64\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6540\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.149967\tvalid_0's pr_auc: 0.455195\n",
      "[200]\tvalid_0's binary_logloss: 0.149268\tvalid_0's pr_auc: 0.457959\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid_0's binary_logloss: 0.149298\tvalid_0's pr_auc: 0.458427\n",
      "Fold 0 PR-AUC: 0.4584\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6564\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.148581\tvalid_0's pr_auc: 0.466067\n",
      "[200]\tvalid_0's binary_logloss: 0.147905\tvalid_0's pr_auc: 0.467357\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's binary_logloss: 0.14802\tvalid_0's pr_auc: 0.46815\n",
      "Fold 1 PR-AUC: 0.4682\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6567\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.150115\tvalid_0's pr_auc: 0.454714\n",
      "[200]\tvalid_0's binary_logloss: 0.149073\tvalid_0's pr_auc: 0.459614\n",
      "[300]\tvalid_0's binary_logloss: 0.149007\tvalid_0's pr_auc: 0.459905\n",
      "[400]\tvalid_0's binary_logloss: 0.149262\tvalid_0's pr_auc: 0.457983\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid_0's binary_logloss: 0.148986\tvalid_0's pr_auc: 0.460117\n",
      "Fold 2 PR-AUC: 0.4601\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6561\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.148247\tvalid_0's pr_auc: 0.466237\n",
      "[200]\tvalid_0's binary_logloss: 0.147526\tvalid_0's pr_auc: 0.468928\n",
      "[300]\tvalid_0's binary_logloss: 0.147462\tvalid_0's pr_auc: 0.469168\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid_0's binary_logloss: 0.147423\tvalid_0's pr_auc: 0.469847\n",
      "Fold 3 PR-AUC: 0.4698\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6564\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.14954\tvalid_0's pr_auc: 0.459393\n",
      "[200]\tvalid_0's binary_logloss: 0.148666\tvalid_0's pr_auc: 0.462739\n",
      "[300]\tvalid_0's binary_logloss: 0.148688\tvalid_0's pr_auc: 0.463094\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's binary_logloss: 0.148591\tvalid_0's pr_auc: 0.463407\n",
      "Fold 4 PR-AUC: 0.4634\n",
      "FINISHED \\ whole score: 0.4638\n",
      "----------------[0] 0.4638----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6587\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.150175\tvalid_0's pr_auc: 0.454826\n",
      "[200]\tvalid_0's binary_logloss: 0.149483\tvalid_0's pr_auc: 0.457915\n",
      "[300]\tvalid_0's binary_logloss: 0.149382\tvalid_0's pr_auc: 0.458983\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's binary_logloss: 0.149297\tvalid_0's pr_auc: 0.459061\n",
      "Fold 0 PR-AUC: 0.4591\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6553\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.149352\tvalid_0's pr_auc: 0.461533\n",
      "[200]\tvalid_0's binary_logloss: 0.148676\tvalid_0's pr_auc: 0.464772\n",
      "[300]\tvalid_0's binary_logloss: 0.148578\tvalid_0's pr_auc: 0.464966\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid_0's binary_logloss: 0.148597\tvalid_0's pr_auc: 0.465207\n",
      "Fold 1 PR-AUC: 0.4652\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6558\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.149257\tvalid_0's pr_auc: 0.459108\n",
      "[200]\tvalid_0's binary_logloss: 0.148765\tvalid_0's pr_auc: 0.461342\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's binary_logloss: 0.148775\tvalid_0's pr_auc: 0.461916\n",
      "Fold 2 PR-AUC: 0.4619\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6555\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.149568\tvalid_0's pr_auc: 0.460151\n",
      "[200]\tvalid_0's binary_logloss: 0.148876\tvalid_0's pr_auc: 0.462369\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid_0's binary_logloss: 0.148885\tvalid_0's pr_auc: 0.462552\n",
      "Fold 3 PR-AUC: 0.4626\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6584\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.148853\tvalid_0's pr_auc: 0.462739\n",
      "[200]\tvalid_0's binary_logloss: 0.14818\tvalid_0's pr_auc: 0.465585\n",
      "[300]\tvalid_0's binary_logloss: 0.148079\tvalid_0's pr_auc: 0.46584\n",
      "Early stopping, best iteration is:\n",
      "[278]\tvalid_0's binary_logloss: 0.148045\tvalid_0's pr_auc: 0.466081\n",
      "Fold 4 PR-AUC: 0.4661\n",
      "FINISHED \\ whole score: 0.4629\n",
      "----------------[1] 0.4629----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6561\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.149891\tvalid_0's pr_auc: 0.45586\n",
      "[200]\tvalid_0's binary_logloss: 0.148984\tvalid_0's pr_auc: 0.460658\n",
      "[300]\tvalid_0's binary_logloss: 0.149105\tvalid_0's pr_auc: 0.459588\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's binary_logloss: 0.148904\tvalid_0's pr_auc: 0.460952\n",
      "Fold 0 PR-AUC: 0.4610\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772841\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6563\n",
      "[LightGBM] [Info] Number of data points in the train set: 825233, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063488 -> initscore=-2.691319\n",
      "[LightGBM] [Info] Start training from score -2.691319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.150037\tvalid_0's pr_auc: 0.457502\n",
      "[200]\tvalid_0's binary_logloss: 0.149331\tvalid_0's pr_auc: 0.460593\n",
      "[300]\tvalid_0's binary_logloss: 0.149235\tvalid_0's pr_auc: 0.460661\n",
      "Early stopping, best iteration is:\n",
      "[272]\tvalid_0's binary_logloss: 0.149183\tvalid_0's pr_auc: 0.461172\n",
      "Fold 1 PR-AUC: 0.4612\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6565\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.150279\tvalid_0's pr_auc: 0.455935\n",
      "[200]\tvalid_0's binary_logloss: 0.149499\tvalid_0's pr_auc: 0.460135\n",
      "[300]\tvalid_0's binary_logloss: 0.149405\tvalid_0's pr_auc: 0.461036\n",
      "Early stopping, best iteration is:\n",
      "[295]\tvalid_0's binary_logloss: 0.1494\tvalid_0's pr_auc: 0.461055\n",
      "Fold 2 PR-AUC: 0.4611\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6576\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.147752\tvalid_0's pr_auc: 0.471331\n",
      "[200]\tvalid_0's binary_logloss: 0.146884\tvalid_0's pr_auc: 0.474594\n",
      "[300]\tvalid_0's binary_logloss: 0.146679\tvalid_0's pr_auc: 0.474803\n",
      "[400]\tvalid_0's binary_logloss: 0.146808\tvalid_0's pr_auc: 0.474035\n",
      "Early stopping, best iteration is:\n",
      "[312]\tvalid_0's binary_logloss: 0.146617\tvalid_0's pr_auc: 0.474993\n",
      "Fold 3 PR-AUC: 0.4750\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 772842\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6565\n",
      "[LightGBM] [Info] Number of data points in the train set: 825234, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063487 -> initscore=-2.691321\n",
      "[LightGBM] [Info] Start training from score -2.691321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.148525\tvalid_0's pr_auc: 0.463738\n",
      "[200]\tvalid_0's binary_logloss: 0.147738\tvalid_0's pr_auc: 0.466716\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's binary_logloss: 0.147732\tvalid_0's pr_auc: 0.46731\n",
      "Fold 4 PR-AUC: 0.4673\n",
      "FINISHED \\ whole score: 0.4649\n",
      "----------------[2] 0.4649----------------\n",
      "CPU times: user 5h 44min 11s, sys: 11min 47s, total: 5h 55min 58s\n",
      "Wall time: 15min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_subsample:\n",
    "    pred, score = resampling_train_pred()\n",
    "else:\n",
    "    pred, score = train_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/submission_sub.csv\n"
     ]
    }
   ],
   "source": [
    "out_filename = 'submission'\n",
    "if is_time_series:\n",
    "    out_filename = out_filename + '_ts'\n",
    "\n",
    "if is_subsample:\n",
    "    out_filename = out_filename + '_sub'\n",
    "\n",
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "filepath = os.path.join(OUTPUT_DIR, out_filename + '.csv')\n",
    "sub_df.to_csv(filepath, index=False)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feature=78\n",
      "- score=0.4638\n"
     ]
    }
   ],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('../output/submission.csv')\n",
    "# sub_ts = pd.read_csv('../output/submission_ts.csv')\n",
    "# assert len(sub) == len(sub_ts)\n",
    "# sub['target'] = (sub['target'] + sub_ts['target'])/2\n",
    "# sub.to_csv('../output/ensumble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsampling_31: learning_rate:0.1\n",
    "- feature=78\n",
    "- score=0.4638\n",
    "- publicLB= 0.2430\n",
    "- privateLB= 0.2588\n",
    "\n",
    "#### ensumble_30: simple_30+ts_30\n",
    "- publicLB= 0.2393\n",
    "- privateLB= 0.2560\n",
    "\n",
    "#### ts_30: learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.2290\n",
    "- publicLB= 0.2308\n",
    "- privateLB= 0.2463\n",
    "\n",
    "#### simple_30: simple_27相当 learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.3277\n",
    "- publicLB= 0.2430 ★best★\n",
    "- privateLB= 0.2599\n",
    "\n",
    "#### subsampling_29: \n",
    "- feature=78\n",
    "- score=0.6501\n",
    "- publicLB= 0.2398\n",
    "- privateLB= 0.2576\n",
    "\n",
    "#### simple_27: pivot('dayofweek', 'hour_zone')\n",
    "- feature=78\n",
    "- score=0.3273\n",
    "- publicLB= 0.2420\n",
    "- privateLB= 0.2582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
