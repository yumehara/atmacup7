{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "# import optuna.integration.lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "pd.set_option('display.max_Columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_No = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_time_series = True\n",
    "is_subsample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (1997595, 35)\n",
      "test_df (390095, 30)\n",
      "train_feat_df (1997595, 78)\n",
      "test_feat_df (390095, 78)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.f')\n",
    "test_df = pd.read_feather('../input/test.f')\n",
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "\n",
    "train_feat_df = pd.read_feather('../input/train_feat_df_{}.f'.format(preprocess_No))\n",
    "test_feat_df = pd.read_feather('../input/test_feat_df_{}.f'.format(preprocess_No))\n",
    "print('train_feat_df', train_feat_df.shape)\n",
    "print('test_feat_df', test_feat_df.shape)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(train_feat_df.columns)\n",
    "print(feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.01,\n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if is_time_series:\n",
    "#     fold = TimeSeriesSplit(n_splits=5)\n",
    "# else:\n",
    "#     fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = list(fold.split(train_feat_df, y)) \n",
    "\n",
    "# tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lgbm(X, y):\n",
    "    if is_time_series:\n",
    "        fold = TimeSeriesSplit(n_splits=5)\n",
    "    else:\n",
    "        fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = list(fold.split(X, y)) \n",
    "\n",
    "    oof, models, score = train_lgbm(X, y, cv, params=lgbm_param)\n",
    "    return oof, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred():\n",
    "    oof, models, score = kfold_lgbm(train_feat_df, y)\n",
    "    pred_list = []\n",
    "    for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resampling_train_pred():\n",
    "    print(y.value_counts())\n",
    "    negative = y.value_counts()[0]\n",
    "    positive = y.value_counts()[1]\n",
    "    strategy = {0:int(negative/5), 1:positive}\n",
    "\n",
    "    pred_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(3):\n",
    "        rus = RandomUnderSampler(random_state=i*9, sampling_strategy = strategy)\n",
    "        X_resampled, y_resampled = rus.fit_resample(train_feat_df, y)\n",
    "\n",
    "        oof, models, score = kfold_lgbm(X_resampled, y_resampled)\n",
    "        score_list.append(score)\n",
    "\n",
    "        for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        print('----------------[{}] {:.4f}----------------'.format(i, score))\n",
    "\n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    score_ave = np.mean(score_list, axis=0)\n",
    "    return pred, score_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance(models).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 12398, number of negative: 320537\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6282\n",
      "[LightGBM] [Info] Number of data points in the train set: 332935, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037239 -> initscore=-3.252463\n",
      "[LightGBM] [Info] Start training from score -3.252463\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0998008\tvalid_0's pr_auc: 0.265175\n",
      "[200]\tvalid_0's binary_logloss: 0.0925\tvalid_0's pr_auc: 0.27574\n",
      "[300]\tvalid_0's binary_logloss: 0.0897978\tvalid_0's pr_auc: 0.280029\n",
      "[400]\tvalid_0's binary_logloss: 0.0887387\tvalid_0's pr_auc: 0.281929\n",
      "[500]\tvalid_0's binary_logloss: 0.0881686\tvalid_0's pr_auc: 0.28284\n",
      "[600]\tvalid_0's binary_logloss: 0.0879422\tvalid_0's pr_auc: 0.283405\n",
      "[700]\tvalid_0's binary_logloss: 0.0878752\tvalid_0's pr_auc: 0.283702\n",
      "[800]\tvalid_0's binary_logloss: 0.0878121\tvalid_0's pr_auc: 0.283872\n",
      "Early stopping, best iteration is:\n",
      "[770]\tvalid_0's binary_logloss: 0.0878122\tvalid_0's pr_auc: 0.283982\n",
      "Fold 0 PR-AUC: 0.2840\n",
      "[LightGBM] [Info] Number of positive: 21804, number of negative: 644063\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6394\n",
      "[LightGBM] [Info] Number of data points in the train set: 665867, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032745 -> initscore=-3.385703\n",
      "[LightGBM] [Info] Start training from score -3.385703\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.102087\tvalid_0's pr_auc: 0.239656\n",
      "[200]\tvalid_0's binary_logloss: 0.0952777\tvalid_0's pr_auc: 0.249841\n",
      "[300]\tvalid_0's binary_logloss: 0.0926795\tvalid_0's pr_auc: 0.256201\n",
      "[400]\tvalid_0's binary_logloss: 0.0916396\tvalid_0's pr_auc: 0.259226\n",
      "[500]\tvalid_0's binary_logloss: 0.0909537\tvalid_0's pr_auc: 0.261507\n",
      "[600]\tvalid_0's binary_logloss: 0.0905318\tvalid_0's pr_auc: 0.263648\n",
      "[700]\tvalid_0's binary_logloss: 0.0902883\tvalid_0's pr_auc: 0.265064\n",
      "[800]\tvalid_0's binary_logloss: 0.0900942\tvalid_0's pr_auc: 0.266182\n",
      "[900]\tvalid_0's binary_logloss: 0.0899693\tvalid_0's pr_auc: 0.267123\n",
      "[1000]\tvalid_0's binary_logloss: 0.0899052\tvalid_0's pr_auc: 0.267681\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.0899052\tvalid_0's pr_auc: 0.267681\n",
      "Fold 1 PR-AUC: 0.2677\n",
      "[LightGBM] [Info] Number of positive: 31506, number of negative: 967293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6413\n",
      "[LightGBM] [Info] Number of data points in the train set: 998799, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031544 -> initscore=-3.424323\n",
      "[LightGBM] [Info] Start training from score -3.424323\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.109904\tvalid_0's pr_auc: 0.265518\n",
      "[200]\tvalid_0's binary_logloss: 0.10326\tvalid_0's pr_auc: 0.2721\n",
      "[300]\tvalid_0's binary_logloss: 0.10088\tvalid_0's pr_auc: 0.275569\n",
      "[400]\tvalid_0's binary_logloss: 0.0999214\tvalid_0's pr_auc: 0.277908\n",
      "[500]\tvalid_0's binary_logloss: 0.0992927\tvalid_0's pr_auc: 0.279657\n",
      "[600]\tvalid_0's binary_logloss: 0.0989407\tvalid_0's pr_auc: 0.281011\n",
      "[700]\tvalid_0's binary_logloss: 0.0987301\tvalid_0's pr_auc: 0.282223\n",
      "[800]\tvalid_0's binary_logloss: 0.0985798\tvalid_0's pr_auc: 0.282809\n",
      "[900]\tvalid_0's binary_logloss: 0.0984737\tvalid_0's pr_auc: 0.283544\n",
      "[1000]\tvalid_0's binary_logloss: 0.0984143\tvalid_0's pr_auc: 0.28378\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.0984143\tvalid_0's pr_auc: 0.28378\n",
      "Fold 2 PR-AUC: 0.2838\n",
      "[LightGBM] [Info] Number of positive: 42370, number of negative: 1289361\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6505\n",
      "[LightGBM] [Info] Number of data points in the train set: 1331731, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031816 -> initscore=-3.415461\n",
      "[LightGBM] [Info] Start training from score -3.415461\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.115161\tvalid_0's pr_auc: 0.261515\n",
      "[200]\tvalid_0's binary_logloss: 0.108348\tvalid_0's pr_auc: 0.270678\n",
      "[300]\tvalid_0's binary_logloss: 0.105894\tvalid_0's pr_auc: 0.275521\n",
      "[400]\tvalid_0's binary_logloss: 0.104925\tvalid_0's pr_auc: 0.278492\n",
      "[500]\tvalid_0's binary_logloss: 0.104236\tvalid_0's pr_auc: 0.281351\n",
      "[600]\tvalid_0's binary_logloss: 0.103856\tvalid_0's pr_auc: 0.283133\n",
      "[700]\tvalid_0's binary_logloss: 0.103634\tvalid_0's pr_auc: 0.284546\n",
      "[800]\tvalid_0's binary_logloss: 0.103436\tvalid_0's pr_auc: 0.285894\n",
      "[900]\tvalid_0's binary_logloss: 0.103311\tvalid_0's pr_auc: 0.286875\n",
      "[1000]\tvalid_0's binary_logloss: 0.103208\tvalid_0's pr_auc: 0.287643\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.103208\tvalid_0's pr_auc: 0.287643\n",
      "Fold 3 PR-AUC: 0.2876\n",
      "[LightGBM] [Info] Number of positive: 53801, number of negative: 1610862\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6567\n",
      "[LightGBM] [Info] Number of data points in the train set: 1664663, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032319 -> initscore=-3.399233\n",
      "[LightGBM] [Info] Start training from score -3.399233\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.1209\tvalid_0's pr_auc: 0.234974\n",
      "[200]\tvalid_0's binary_logloss: 0.114229\tvalid_0's pr_auc: 0.242095\n",
      "[300]\tvalid_0's binary_logloss: 0.111819\tvalid_0's pr_auc: 0.246171\n",
      "[400]\tvalid_0's binary_logloss: 0.110975\tvalid_0's pr_auc: 0.248231\n",
      "[500]\tvalid_0's binary_logloss: 0.110355\tvalid_0's pr_auc: 0.250127\n",
      "[600]\tvalid_0's binary_logloss: 0.110015\tvalid_0's pr_auc: 0.251854\n",
      "[700]\tvalid_0's binary_logloss: 0.109844\tvalid_0's pr_auc: 0.252935\n",
      "[800]\tvalid_0's binary_logloss: 0.109688\tvalid_0's pr_auc: 0.254084\n",
      "[900]\tvalid_0's binary_logloss: 0.109576\tvalid_0's pr_auc: 0.254902\n",
      "[1000]\tvalid_0's binary_logloss: 0.10951\tvalid_0's pr_auc: 0.255482\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's binary_logloss: 0.109509\tvalid_0's pr_auc: 0.255486\n",
      "Fold 4 PR-AUC: 0.2555\n",
      "FINISHED \\ whole score: 0.2290\n",
      "CPU times: user 8h 24min 48s, sys: 17min 55s, total: 8h 42min 44s\n",
      "Wall time: 21min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_subsample:\n",
    "    pred, score = resampling_train_pred()\n",
    "else:\n",
    "    pred, score = train_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/submission_ts.csv\n"
     ]
    }
   ],
   "source": [
    "out_filename = 'submission'\n",
    "if is_time_series:\n",
    "    out_filename = out_filename + '_ts'\n",
    "\n",
    "if is_subsample:\n",
    "    out_filename = out_filename + '_sub'\n",
    "\n",
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "filepath = os.path.join(OUTPUT_DIR, out_filename + '.csv')\n",
    "sub_df.to_csv(filepath, index=False)\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feature=78\n",
      "- score=0.2290\n"
     ]
    }
   ],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('../output/submission.csv')\n",
    "# sub_ts = pd.read_csv('../output/submission_ts.csv')\n",
    "# assert len(sub) == len(sub_ts)\n",
    "# sub['target'] = (sub['target'] + sub_ts['target'])/2\n",
    "# sub.to_csv('../output/ensumble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ensumble_30: simple_30+ts_30\n",
    "- publicLB= 0.2393\n",
    "- privateLB= 0.2560\n",
    "\n",
    "#### ts_30: learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.2290\n",
    "- publicLB= 0.2308\n",
    "- privateLB= 0.2463\n",
    "\n",
    "#### simple_30: simple_27相当 learning_rate:0.01\n",
    "- feature=78\n",
    "- score=0.3277\n",
    "- publicLB= 0.2430 ★best★\n",
    "- privateLB= 0.2599\n",
    "\n",
    "#### subsampling_29: \n",
    "- feature=78\n",
    "- score=0.6501\n",
    "- publicLB= 0.2398\n",
    "- privateLB= 0.2576\n",
    "\n",
    "#### simple_27: pivot('dayofweek', 'hour_zone')\n",
    "- feature=78\n",
    "- score=0.3273\n",
    "- publicLB= 0.2420\n",
    "- privateLB= 0.2582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
