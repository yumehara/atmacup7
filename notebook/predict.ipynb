{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "# import optuna.integration.lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "pd.set_option('display.max_Columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_time_series = False\n",
    "is_subsample = True\n",
    "is_ensumble = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (1997595, 35)\n",
      "test_df (390095, 30)\n",
      "train_feat_df (1997595, 78)\n",
      "test_feat_df (390095, 78)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.f')\n",
    "test_df = pd.read_feather('../input/test.f')\n",
    "print('train_df', train_df.shape)\n",
    "print('test_df', test_df.shape)\n",
    "\n",
    "train_feat_df = pd.read_feather('../input/train_feat_df.f')\n",
    "test_feat_df = pd.read_feather('../input/test_feat_df.f')\n",
    "print('train_feat_df', train_feat_df.shape)\n",
    "print('test_feat_df', test_feat_df.shape)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(train_feat_df.columns)\n",
    "print(feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(y_pred, y_true):\n",
    "    \"\"\"lightGBM の round ごとに PR-AUC を計算する用\"\"\"\n",
    "    score = average_precision_score(y_true.get_label(), y_pred)\n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param = {\n",
    "    'objective' : 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'seed' : 0,\n",
    "    'learning_rate':  0.1,\n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, params: dict, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    # float にしないと悲しい事件が起こるのでそこだけ注意\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "        \n",
    "        lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "        lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "        \n",
    "        lgbm_model = lgbm.train(params, \n",
    "                                                    lgbm_train, \n",
    "                                                    valid_sets=lgbm_eval,\n",
    "                                                    num_boost_round=1000,\n",
    "                                                    early_stopping_rounds=verbose,\n",
    "                                                    feval=pr_auc,\n",
    "                                                    verbose_eval=verbose)\n",
    "        y_pred = lgbm_model.predict(x_valid, num_iteration=lgbm_model.best_iteration)\n",
    "        \n",
    "        oof_pred[idx_valid] = y_pred\n",
    "        models.append(lgbm_model)\n",
    "\n",
    "        print(f'Fold {i} PR-AUC: {average_precision_score(y_valid, y_pred):.4f}')\n",
    "\n",
    "    score = average_precision_score(y, oof_pred)\n",
    "    print('FINISHED \\ whole score: {:.4f}'.format(score))\n",
    "    return oof_pred, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(X, y, cv, params, verbose=100):\n",
    "    idx_train, idx_valid = cv[0]\n",
    "    x_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    x_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    lgbm_train = lgbm.Dataset(x_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n",
    "    \n",
    "    best_params, tuning_history = dict(), list()\n",
    "    best = lgbm.train(params,\n",
    "                                  lgbm_train,\n",
    "                                  valid_sets=lgbm_eval,\n",
    "                                  num_boost_round=1000,\n",
    "                                  early_stopping_rounds=verbose,\n",
    "                                  feval=pr_auc,\n",
    "                                  verbose_eval=0)\n",
    "    print('Best Params:', best.params)\n",
    "    print('Best Iteration:', best.best_iteration)\n",
    "    print('Best Score:', best.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if is_time_series:\n",
    "#     fold = TimeSeriesSplit(n_splits=5)\n",
    "# else:\n",
    "#     fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv = list(fold.split(train_feat_df, y)) \n",
    "\n",
    "# tuning_lgbm(train_feat_df, y, cv, params=lgbm_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lgbm(X, y):\n",
    "    if is_time_series:\n",
    "        fold = TimeSeriesSplit(n_splits=5)\n",
    "    else:\n",
    "        fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = list(fold.split(X, y)) \n",
    "\n",
    "    oof, models, score = train_lgbm(X, y, cv, params=lgbm_param)\n",
    "    return oof, models, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred():\n",
    "    oof, models, score = kfold_lgbm(train_feat_df, y)\n",
    "    pred_list = []\n",
    "    for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resampling_train_pred():\n",
    "    print(y.value_counts())\n",
    "    negative = y.value_counts()[0]\n",
    "    positive = y.value_counts()[1]\n",
    "    strategy = {0:int(negative/5), 1:positive}\n",
    "\n",
    "    pred_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(3):\n",
    "        rus = RandomUnderSampler(random_state=i*9, sampling_strategy = strategy)\n",
    "        X_resampled, y_resampled = rus.fit_resample(train_feat_df, y)\n",
    "\n",
    "        oof, models, score = kfold_lgbm(X_resampled, y_resampled)\n",
    "        score_list.append(score)\n",
    "\n",
    "        for model in models:\n",
    "            pred = model.predict(test_feat_df, num_iteration = model.best_iteration)\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        print('----------------[{}] {}----------------'.format(i, score))\n",
    "\n",
    "    pred = np.mean(pred_list, axis=0)\n",
    "    score_ave = np.mean(score_list, axis=0)\n",
    "    return pred, score_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(model):\n",
    "    fi = model.feature_importance()\n",
    "    fn = model.feature_name()\n",
    "    df_feature_importance = pd.DataFrame({'name':fn, 'imp':fi})\n",
    "    df_feature_importance.sort_values('imp', inplace=True)\n",
    "    return df_feature_importance\n",
    "\n",
    "def feature_importance(models):\n",
    "    fi = pd.DataFrame(columns=['name'])\n",
    "    for i, model in enumerate(models):\n",
    "        fi_tmp = feat_imp(model)\n",
    "        colname = 'imp_{}'.format(i)\n",
    "        fi_tmp.rename(columns={'imp': colname}, inplace=True)\n",
    "        fi = pd.merge(fi, fi_tmp, on=['name'], how='outer')\n",
    "    fi['sum'] = fi.sum(axis=1)\n",
    "    return fi.sort_values(['sum'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance(models).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1932105\n",
      "1      65490\n",
      "Name: target, dtype: int64\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309136\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6550\n",
      "[LightGBM] [Info] Number of data points in the train set: 361528, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775027\n",
      "[LightGBM] [Info] Start training from score -1.775027\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.244174\tvalid_0's pr_auc: 0.646025\n",
      "[200]\tvalid_0's binary_logloss: 0.24261\tvalid_0's pr_auc: 0.648503\n",
      "Early stopping, best iteration is:\n",
      "[187]\tvalid_0's binary_logloss: 0.242609\tvalid_0's pr_auc: 0.64888\n",
      "Fold 0 PR-AUC: 0.6489\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6556\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.241558\tvalid_0's pr_auc: 0.652802\n",
      "[200]\tvalid_0's binary_logloss: 0.240332\tvalid_0's pr_auc: 0.654153\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's binary_logloss: 0.240387\tvalid_0's pr_auc: 0.654434\n",
      "Fold 1 PR-AUC: 0.6544\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6551\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.241661\tvalid_0's pr_auc: 0.652173\n",
      "[200]\tvalid_0's binary_logloss: 0.240399\tvalid_0's pr_auc: 0.654521\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid_0's binary_logloss: 0.240295\tvalid_0's pr_auc: 0.655387\n",
      "Fold 2 PR-AUC: 0.6554\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6572\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.243557\tvalid_0's pr_auc: 0.646508\n",
      "[200]\tvalid_0's binary_logloss: 0.242496\tvalid_0's pr_auc: 0.648061\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's binary_logloss: 0.242525\tvalid_0's pr_auc: 0.648864\n",
      "Fold 3 PR-AUC: 0.6489\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6554\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.241683\tvalid_0's pr_auc: 0.653574\n",
      "[200]\tvalid_0's binary_logloss: 0.240272\tvalid_0's pr_auc: 0.65565\n",
      "[300]\tvalid_0's binary_logloss: 0.240288\tvalid_0's pr_auc: 0.655792\n",
      "Early stopping, best iteration is:\n",
      "[268]\tvalid_0's binary_logloss: 0.24014\tvalid_0's pr_auc: 0.656373\n",
      "Fold 4 PR-AUC: 0.6564\n",
      "FINISHED \\ whole score: 0.6527\n",
      "----------------[0] 0.6527135101773949----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309136\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6563\n",
      "[LightGBM] [Info] Number of data points in the train set: 361528, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775027\n",
      "[LightGBM] [Info] Start training from score -1.775027\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.241708\tvalid_0's pr_auc: 0.651841\n",
      "[200]\tvalid_0's binary_logloss: 0.240157\tvalid_0's pr_auc: 0.654292\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's binary_logloss: 0.240137\tvalid_0's pr_auc: 0.654236\n",
      "Fold 0 PR-AUC: 0.6542\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6576\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.243573\tvalid_0's pr_auc: 0.643827\n",
      "[200]\tvalid_0's binary_logloss: 0.242108\tvalid_0's pr_auc: 0.647627\n",
      "[300]\tvalid_0's binary_logloss: 0.242295\tvalid_0's pr_auc: 0.647049\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's binary_logloss: 0.242084\tvalid_0's pr_auc: 0.647751\n",
      "Fold 1 PR-AUC: 0.6478\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6567\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.240965\tvalid_0's pr_auc: 0.650134\n",
      "[200]\tvalid_0's binary_logloss: 0.239624\tvalid_0's pr_auc: 0.652168\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's binary_logloss: 0.239913\tvalid_0's pr_auc: 0.652286\n",
      "Fold 2 PR-AUC: 0.6523\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6577\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.242295\tvalid_0's pr_auc: 0.651613\n",
      "[200]\tvalid_0's binary_logloss: 0.241079\tvalid_0's pr_auc: 0.653932\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's binary_logloss: 0.241269\tvalid_0's pr_auc: 0.654346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 PR-AUC: 0.6543\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6578\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.242155\tvalid_0's pr_auc: 0.651096\n",
      "[200]\tvalid_0's binary_logloss: 0.241046\tvalid_0's pr_auc: 0.652171\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's binary_logloss: 0.241244\tvalid_0's pr_auc: 0.652872\n",
      "Fold 4 PR-AUC: 0.6529\n",
      "FINISHED \\ whole score: 0.6521\n",
      "----------------[1] 0.6520891689634492----------------\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309136\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6571\n",
      "[LightGBM] [Info] Number of data points in the train set: 361528, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775027\n",
      "[LightGBM] [Info] Start training from score -1.775027\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.243987\tvalid_0's pr_auc: 0.646289\n",
      "[200]\tvalid_0's binary_logloss: 0.242905\tvalid_0's pr_auc: 0.64828\n",
      "[300]\tvalid_0's binary_logloss: 0.242834\tvalid_0's pr_auc: 0.647918\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.242706\tvalid_0's pr_auc: 0.64866\n",
      "Fold 0 PR-AUC: 0.6487\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6574\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.244323\tvalid_0's pr_auc: 0.645853\n",
      "[200]\tvalid_0's binary_logloss: 0.243337\tvalid_0's pr_auc: 0.647037\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid_0's binary_logloss: 0.243455\tvalid_0's pr_auc: 0.647594\n",
      "Fold 1 PR-AUC: 0.6476\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6581\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.239938\tvalid_0's pr_auc: 0.653353\n",
      "[200]\tvalid_0's binary_logloss: 0.238564\tvalid_0's pr_auc: 0.656177\n",
      "[300]\tvalid_0's binary_logloss: 0.238892\tvalid_0's pr_auc: 0.655039\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.238529\tvalid_0's pr_auc: 0.656333\n",
      "Fold 2 PR-AUC: 0.6563\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027347 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6570\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.244046\tvalid_0's pr_auc: 0.641546\n",
      "[200]\tvalid_0's binary_logloss: 0.242721\tvalid_0's pr_auc: 0.644769\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's binary_logloss: 0.24271\tvalid_0's pr_auc: 0.644893\n",
      "Fold 3 PR-AUC: 0.6449\n",
      "[LightGBM] [Info] Number of positive: 52392, number of negative: 309137\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6564\n",
      "[LightGBM] [Info] Number of data points in the train set: 361529, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.144918 -> initscore=-1.775031\n",
      "[LightGBM] [Info] Start training from score -1.775031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.241593\tvalid_0's pr_auc: 0.651812\n",
      "[200]\tvalid_0's binary_logloss: 0.240329\tvalid_0's pr_auc: 0.653919\n",
      "[300]\tvalid_0's binary_logloss: 0.240657\tvalid_0's pr_auc: 0.652841\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's binary_logloss: 0.240321\tvalid_0's pr_auc: 0.653976\n",
      "Fold 4 PR-AUC: 0.6540\n",
      "FINISHED \\ whole score: 0.6501\n",
      "----------------[2] 0.6501290621445753----------------\n",
      "CPU times: user 2h 16min 5s, sys: 1min 57s, total: 2h 18min 3s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_subsample:\n",
    "    pred, score = resampling_train_pred()\n",
    "else:\n",
    "    pred, score = train_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred) == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'submission'\n",
    "if is_time_series:\n",
    "    out_filename = out_filename + '_ts'\n",
    "\n",
    "if is_subsample:\n",
    "    out_filename = out_filename + '_sub'\n",
    "\n",
    "sub_df = pd.DataFrame({ 'target': pred })\n",
    "sub_df.to_csv(os.path.join(OUTPUT_DIR, out_filename + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- feature=78\n",
      "- score=0.6516\n"
     ]
    }
   ],
   "source": [
    "print('- feature={}'.format(feature_count))\n",
    "print('- score={:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_ensumble:\n",
    "    sub = pd.read_csv('../output/submission.csv')\n",
    "    sub_ts = pd.read_csv('../output/submission_ts.csv')\n",
    "    assert len(sub) == len(sub_ts)\n",
    "    sub['target'] = (sub['target'] + sub_ts['target'])/2\n",
    "    sub.to_csv('../output/ensumble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsampling_29: \n",
    "- feature=78\n",
    "- score=0.6501\n",
    "- publicLB= 0.2398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning\n",
    "```\n",
    "Best Params: {\n",
    "    'objective': 'binary', \n",
    "    'boosting_type': 'gbdt', \n",
    "    'seed': 0, \n",
    "    'learning_rate': 0.1, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 6.419345380049121e-07, \n",
    "    'lambda_l2': 8.432801302426078, \n",
    "    'num_leaves': 212, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.9907178796872467, \n",
    "    'bagging_freq': 2, \n",
    "    'min_child_samples': 100, \n",
    "    'num_iterations': 1000, \n",
    "    'early_stopping_round': 100\n",
    "}\n",
    "Best Iteration: 245\n",
    "Best Score: 'pr_auc', 0.22382995580267329\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
